{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PFA-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor, zeros, zeros_like, cat, eye, mean, sum as tsum, arange, cuda, load\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DistributedSampler, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets.folder import pil_loader\n",
    "from collections import OrderedDict\n",
    "from matplotlib import pyplot as plt\n",
    "#from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "from apex import amp\n",
    "from apex.parallel import DistributedDataParallel\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "import os\n",
    "import math\n",
    "import h5py\n",
    "import torch\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age2group(age, age_group):\n",
    "    if isinstance(age, np.ndarray):\n",
    "        groups = np.zeros_like(age)\n",
    "    else:\n",
    "        groups = zeros_like(age).to(age.device)\n",
    "\n",
    "    if age_group == 4:\n",
    "        section = [30, 40, 50]\n",
    "    elif age_group == 5:\n",
    "        section = [20, 30, 40, 50]\n",
    "    elif age_group == 7:\n",
    "        section = [10, 20, 30, 40, 50, 60]\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    for i, thresh in enumerate(section, 1):\n",
    "        groups[age > thresh] = i\n",
    "        \n",
    "    return groups[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CACD_Dataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            root_dir='materials',\n",
    "            dataset_name='cacd',\n",
    "            age_group=4,\n",
    "            train=False,\n",
    "            source=0,\n",
    "            max_iter=200000,\n",
    "            batch_size=64,\n",
    "            transforms=None\n",
    "    ):\n",
    "        self.root_dir = root_dir\n",
    "        self.dataset_name = dataset_name\n",
    "        self.age_group = age_group\n",
    "        self.train = train\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.total_pairs = batch_size * max_iter\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self._load_meta_data()\n",
    "\n",
    "        self.mean_ages = np.array(\n",
    "            [np.mean(self.ages[self.age_groups == i])\n",
    "            for i in range(self.age_group)]\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        self.label_group_images = []\n",
    "        self.label_group_ages = []\n",
    "\n",
    "        for i in range(self.age_group):\n",
    "            self.label_group_images.append(\n",
    "                self.image_names[self.age_groups == i].tolist())\n",
    "            self.label_group_ages.append(\n",
    "                self.ages[self.age_groups == i].astype(np.float32).tolist())\n",
    "\n",
    "        self.target_labels = np.random.randint(source + 1, self.age_group, self.total_pairs)\n",
    "\n",
    "        pairs = np.array(list(itertools.combinations(range(age_group), 2)))\n",
    "        p = [1, 1, 1, 0.5, 0.5, 0.5]\n",
    "        p = np.array(p) / np.sum(p)\n",
    "        pairs = pairs[np.random.choice(range(len(pairs)), self.total_pairs, p=p), :]\n",
    "        source_labels, target_labels = pairs[:, 0], pairs[:, 1]\n",
    "        self.source_labels = source_labels\n",
    "        self.target_labels = target_labels\n",
    "\n",
    "        self.true_labels = np.random.randint(0, self.age_group, self.total_pairs)\n",
    "\n",
    "    def _load_meta_data(self):\n",
    "        meta = h5py.File(os.path.join(self.root_dir, f\"{self.dataset_name}.mat\"), 'r')\n",
    "\n",
    "        self.ages = meta['celebrityImageData']['age'][0,:]\n",
    "        self.age_groups = np.asanyarray([age2group(np.asanyarray([age]), self.age_group) for age in self.ages])\n",
    "        self.image_names = np.asanyarray(\n",
    "            [''.join(chr(i[0])\n",
    "                     for i in hdf5_object)\n",
    "                     for hdf5_object in [meta[hdf5_object_reference][:] \n",
    "                     for hdf5_object_references in meta['celebrityImageData']['name']\n",
    "                     for hdf5_object_reference in hdf5_object_references]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        if len(self.ages) == len(self.image_names) and len(self.image_names) == len(self.age_groups):\n",
    "            return len(self.ages)\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source_label = self.source_labels[idx]\n",
    "        target_label = self.target_labels[idx]\n",
    "        true_label = self.true_labels[idx]\n",
    "\n",
    "        source_img = transforms.ToTensor()(pil_loader(os.path.join(self.root_dir, self.dataset_name, random.choice(self.label_group_images[source_label]))).resize((248,248)))\n",
    "\n",
    "        index = random.randint(0, len(self.label_group_images[true_label]) - 1)\n",
    "        true_img = transforms.ToTensor()(pil_loader(os.path.join(self.root_dir, self.dataset_name, self.label_group_images[true_label][index])).resize((248,248)))\n",
    "        true_age = self.label_group_ages[true_label][index]\n",
    "        mean_age = self.mean_ages[target_label]\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            source_img = self.transforms(source_img)\n",
    "            true_img = self.transforms(true_img)\n",
    "\n",
    "        return source_img, true_img, source_label, target_label, true_label, true_age, mean_age\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPrefetcher():\n",
    "    def __init__(self, loader, *norm_index):\n",
    "        self.loader = iter(loader)\n",
    "        self.normlize = lambda x: x.sub_(0.5).div_(0.5)\n",
    "        self.norm_index = norm_index\n",
    "        self.stream = cuda.Stream()\n",
    "        self.preload()\n",
    "\n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_input = next(self.loader)\n",
    "        except StopIteration:\n",
    "            self.next_input = None\n",
    "            return\n",
    "        with cuda.stream(self.stream):\n",
    "            self.next_input = [\n",
    "                self.normlize(x.cuda(non_blocking=True)) if i in self.norm_index else x.cuda(non_blocking=True)\n",
    "                for i, x in enumerate(self.next_input)\n",
    "            ]\n",
    "\n",
    "    def next(self):\n",
    "        cuda.current_stream().wait_stream(self.stream)\n",
    "        input = self.next_input\n",
    "        self.preload()\n",
    "        return input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PFA-GAN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, age_group, conv_dim=64, repeat_num=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.age_group = age_group\n",
    "        self.conv_dim = conv_dim\n",
    "        self.repeat_num = repeat_num\n",
    "        self._init_model()\n",
    "\n",
    "    def _init_model(self):\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            3,\n",
    "            self.conv_dim,\n",
    "            kernel_size=4,\n",
    "            stride=2,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "        layers = []\n",
    "        nf_mult = 1\n",
    "\n",
    "        # gradually increase the number of filters\n",
    "        for n in range(1, self.repeat_num):\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** n, 8)\n",
    "            \n",
    "            layers += [\n",
    "                nn.utils.spectral_norm(\n",
    "                    nn.Conv2d(\n",
    "                        in_channels=self.conv_dim * nf_mult_prev + (self.age_group if n == 1 else 0),\n",
    "                        out_channels=self.conv_dim * nf_mult,\n",
    "                        kernel_size=4,\n",
    "                        stride=2,\n",
    "                        padding=1,\n",
    "                        bias=True\n",
    "                    )\n",
    "                ),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "        \n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** self.repeat_num, 8)\n",
    "        \n",
    "        layers += [\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.conv_dim * nf_mult_prev,\n",
    "                out_channels=self.conv_dim * nf_mult,\n",
    "                kernel_size=4,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(self.conv_dim * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.conv_dim * nf_mult,\n",
    "                out_channels=1,\n",
    "                kernel_size=4,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        self.main = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, inputs, condition):\n",
    "        x = F.leaky_relu(self.conv1(inputs), 0.2, inplace=True)\n",
    "        condition = self._group2feature(\n",
    "            condition,\n",
    "            feature_size=x.size(2),\n",
    "            age_group=self.age_group\n",
    "        ).to(x)\n",
    "        return self.main(cat([x, condition], dim=1))\n",
    "\n",
    "    def _group2feature(self, group, age_group, feature_size):\n",
    "        onehot = self._group2onehot(\n",
    "            group, \n",
    "            age_group\n",
    "        )\n",
    "        return onehot.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, feature_size, feature_size).to(device)\n",
    "\n",
    "    def _group2onehot(self, groups, age_group):\n",
    "        code = eye(age_group).to(device)[groups.squeeze()]\n",
    "        if len(code.size()) > 1:\n",
    "            return code\n",
    "        return code.unsqueeze(0).to(device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResidualBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.n_channels = channels\n",
    "        self._init_model()\n",
    "    \n",
    "    def _init_model(self):\n",
    "        layers = [\n",
    "            nn.Conv2d(\n",
    "                self.n_channels,\n",
    "                self.n_channels,\n",
    "                3,\n",
    "                1,\n",
    "                1\n",
    "            ),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(\n",
    "                self.n_channels,\n",
    "                self.n_channels,\n",
    "                3,\n",
    "                1,\n",
    "                1\n",
    "            ),\n",
    "            nn.BatchNorm2d(self.n_channels),\n",
    "        ]\n",
    "\n",
    "        self.main = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.main(x)\n",
    "        return F.leaky_relu(residual + x, 0.2, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorSubNetwork(nn.Module):\n",
    "    def __init__(self, in_channels=3, n_residual_blocks=4):\n",
    "        super(GeneratorSubNetwork, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.n_residual_blocks = n_residual_blocks\n",
    "\n",
    "        self._init_model()\n",
    "    \n",
    "    def _init_model(self):\n",
    "        layers = [\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.in_channels,\n",
    "                out_channels=32,\n",
    "                kernel_size=9,\n",
    "                stride=1,\n",
    "                padding=4\n",
    "            ),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                #out_channels=128,\n",
    "                out_channels=32,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1\n",
    "            ),\n",
    "            #nn.InstanceNorm2d(128),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "\n",
    "        for _ in range(self.n_residual_blocks):\n",
    "            #layers.append(ResidualBlock(128))\n",
    "            layers.append(ResidualBlock(32))\n",
    "\n",
    "        layers.extend([\n",
    "            nn.ConvTranspose2d(\n",
    "                #in_channels=128,\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=64,\n",
    "                out_channels=32,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=3,\n",
    "                kernel_size=9,\n",
    "                stride=1,\n",
    "                padding=4\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "        self.main = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, age_group, n_residual_blocks=4):\n",
    "        super(Generator, self).__init__()\n",
    "        self.age_group = age_group\n",
    "        self._init_model(n_residual_blocks)\n",
    "    \n",
    "    def _init_model(self, n_residual_blocks):\n",
    "        self.sub_networks = nn.ModuleList()\n",
    "\n",
    "        for _ in range(self.age_group - 1):\n",
    "            self.sub_networks.append(GeneratorSubNetwork(\n",
    "                in_channels=3,\n",
    "                n_residual_blocks=n_residual_blocks\n",
    "            ))\n",
    "    \n",
    "    def forward(self, x, source_label: Tensor, target_label: Tensor):\n",
    "        condition = self._pfa_encoding(source_label, target_label, self.age_group)\n",
    "        for i in range(self.age_group - 1):\n",
    "            aging_effects = self.sub_networks[i](x)\n",
    "            x = x + aging_effects * condition[:, i]\n",
    "        return x\n",
    "\n",
    "    def _pfa_encoding(self, source, target, age_group):\n",
    "        source, target = source.long(), target.long()\n",
    "        code = zeros((source.size(0), age_group - 1, 1, 1, 1)).to(source)\n",
    "        for i in range(source.size(0)):\n",
    "            code[i, source[i]: target[i], ...] = 1\n",
    "        return code\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxiliaryAgeClassifier(nn.Module):\n",
    "    def __init__(self, conv_dim=64, channels=3, classes=101):\n",
    "        super(AuxiliaryAgeClassifier, self).__init__()\n",
    "\n",
    "        self.conv_dim = conv_dim\n",
    "        self.channels = channels\n",
    "        self.classes = classes\n",
    "\n",
    "        self._init_model()\n",
    "\n",
    "    def _add_vgg_block(self, in_channels, out_channels, more=False):\n",
    "        layers = [\n",
    "            (\n",
    "                'conv1',\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                    padding=1\n",
    "                )\n",
    "            ),\n",
    "            (\n",
    "                'relu1',\n",
    "                nn.ReLU(inplace=True)\n",
    "            ),\n",
    "            (\n",
    "                'conv2',\n",
    "                nn.Conv2d(\n",
    "                    out_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                    padding=1\n",
    "                )\n",
    "            ),\n",
    "            (\n",
    "                'relu2',\n",
    "                nn.ReLU(inplace=True)\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        if more:\n",
    "            layers.extend([\n",
    "                (\n",
    "                    'conv3',\n",
    "                    nn.Conv2d(\n",
    "                        out_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=3,\n",
    "                        stride=1,\n",
    "                        padding=1\n",
    "                    )\n",
    "                ),\n",
    "                (\n",
    "                    'relu3',\n",
    "                    nn.ReLU(inplace=True)\n",
    "                ),\n",
    "            ])\n",
    "\n",
    "        layers.append(('maxpool', nn.MaxPool2d(kernel_size=2, stride=2)))\n",
    "\n",
    "        return nn.Sequential(OrderedDict(layers))\n",
    "\n",
    "    def _init_model(self):\n",
    "        self.conv = nn.Sequential(\n",
    "            self._add_vgg_block(self.channels, self.conv_dim),\n",
    "            self._add_vgg_block(self.conv_dim, self.conv_dim*2),\n",
    "            self._add_vgg_block(self.conv_dim*2, self.conv_dim*4, True),\n",
    "            self._add_vgg_block(self.conv_dim*4, self.conv_dim*8, True),\n",
    "            self._add_vgg_block(self.conv_dim*8, self.conv_dim*8, True),\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear((self.conv_dim*8)*7*7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.cls = nn.Linear(4096, self.classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward operation of the net.\n",
    "        \"\"\"\n",
    "        in_size = x.shape[0]\n",
    "        x = self.conv(x)\n",
    "        x = x.view(in_size, -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.cls(x)\n",
    "        #return F.softmax(x, dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PFA-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PFA_GAN(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            alpha,\n",
    "            pix_loss_weight,\n",
    "            gan_loss_weight,\n",
    "            id_loss_weight,\n",
    "            age_loss_weight,\n",
    "            age_group=4,\n",
    "            image_size=256,\n",
    "            pretrained_image_size=256,\n",
    "            init_lr=1e-4,\n",
    "            restore_iter=0,\n",
    "            max_iter=200000,\n",
    "            save_iter=2000,\n",
    "            decay_pix_factor=0,\n",
    "            decay_pix_n=2000,\n",
    "            num_workers=0,\n",
    "            batch_size=64):\n",
    "        self.age_group = age_group\n",
    "        self.image_size = image_size\n",
    "        self.pretrained_image_size = pretrained_image_size\n",
    "        self.init_lr = init_lr\n",
    "        self.restore_iter = restore_iter\n",
    "        self.max_iter = max_iter\n",
    "        self.save_iter = save_iter\n",
    "        self.pix_loss_weight = pix_loss_weight\n",
    "        self.decay_pix_factor = decay_pix_factor\n",
    "        self.decay_pix_n = decay_pix_n\n",
    "        self.gan_loss_weight = gan_loss_weight\n",
    "        self.alpha = alpha\n",
    "        self.id_loss_weight = id_loss_weight\n",
    "        self.age_loss_weight = age_loss_weight\n",
    "        self.num_workers = num_workers\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.prefetcher = self._get_train_loader()\n",
    "\n",
    "        self._init_model()\n",
    "    \n",
    "    def fit(self):\n",
    "        for n_iter in range(self.restore_iter + 1, self.max_iter + 1):\n",
    "            inputs = self.prefetcher.next()\n",
    "            self.train(inputs, n_iter=n_iter)\n",
    "            if n_iter % self.save_iter == 0 or n_iter == self.max_iter:\n",
    "                pass\n",
    "\n",
    "    def train(self, inputs, n_iter):\n",
    "        source_img, true_img, source_label, target_label, true_label, true_age, mean_age = inputs\n",
    "        source_img, true_img, source_label, target_label, true_label, true_age, mean_age = source_img.to(device), true_img.to(device), source_label.to(device), target_label.to(device), true_label.to(device), true_age.to(device), mean_age.to(device)\n",
    "        \n",
    "        self.generator.train()\n",
    "        self.discriminator.train()\n",
    "\n",
    "        if self.image_size < self.pretrained_image_size:\n",
    "            source_img_small = F.interpolate(source_img, self.image_size)\n",
    "            true_img_small = F.interpolate(true_img, self.image_size)\n",
    "        else:\n",
    "            source_img_small = source_img\n",
    "            true_img_small = true_img\n",
    "        \n",
    "        g_source = self.generator(source_img_small, source_label, target_label)\n",
    "        \n",
    "        if self.image_size < self.pretrained_image_size:\n",
    "            g_source_pretrained = F.interpolate(g_source, self.pretrained_image_size)\n",
    "        else:\n",
    "            g_source_pretrained = g_source\n",
    "\n",
    "        ###########################\n",
    "        # Train Discriminator\n",
    "        ###########################\n",
    "        self.optimizer_discriminator.zero_grad()\n",
    "        d1_logit = self.discriminator(true_img_small, true_label)\n",
    "        # d2_logit = self.discriminator(true_img, source_label)\n",
    "        d3_logit = self.discriminator(g_source.detach(), target_label)\n",
    "\n",
    "        # d_loss = 0.5 * (ls_gan(d1_logit, 1.) + ls_gan(d2_logit, 0.) + ls_gan(d3_logit, 0.))\n",
    "        d_loss = 0.5 * (self._ls_gan(d1_logit, 1.) + self._ls_gan(d3_logit, 0.))\n",
    "\n",
    "        # COMMENT IN \n",
    "        with amp.scale_loss(d_loss, self.optimizer_discriminator) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        self.optimizer_discriminator.step()\n",
    "\n",
    "        ###########################\n",
    "        # Train Generator\n",
    "        ###########################\n",
    "        self.optimizer_generator.zero_grad()\n",
    "\n",
    "        ###########################\n",
    "        # GAN Loss\n",
    "        ###########################\n",
    "        gan_logit = self.discriminator(g_source, target_label)\n",
    "        g_loss = self._ls_gan(gan_logit, 1.)\n",
    "\n",
    "        ###########################\n",
    "        # Age Loss\n",
    "        ###########################\n",
    "        age_loss = self._age_criterion(g_source_pretrained, mean_age)\n",
    "\n",
    "        ###########################\n",
    "        # L1 Loss\n",
    "        ###########################\n",
    "        l1_loss = F.l1_loss(g_source_pretrained, source_img)\n",
    "\n",
    "        ###########################\n",
    "        # SSIM loss\n",
    "        ###########################\n",
    "        ssim_loss = self._compute_ssim_loss(g_source_pretrained, source_img, window_size=10)\n",
    "\n",
    "        ###########################\n",
    "        # ID Loss\n",
    "        ###########################\n",
    "        id_loss = F.mse_loss(\n",
    "            self._extract_vgg_face(g_source_pretrained),\n",
    "            self._extract_vgg_face(source_img)\n",
    "        )\n",
    "\n",
    "        pix_loss_weight = max(\n",
    "            self.pix_loss_weight,\n",
    "            self.pix_loss_weight * (self.decay_pix_factor ** (n_iter // self.decay_pix_n))\n",
    "        )\n",
    "\n",
    "        total_loss = \\\n",
    "            g_loss * self.gan_loss_weight + \\\n",
    "            (l1_loss * (1 - self.alpha) + ssim_loss * self.alpha) * pix_loss_weight + \\\n",
    "            id_loss * self.id_loss_weight + \\\n",
    "            age_loss * self.age_loss_weight\n",
    "        \n",
    "        # COMMENT IN \n",
    "        with amp.scale_loss(total_loss, self.optimizer_generator) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        self.optimizer_generator.step()\n",
    "\n",
    "    def _ls_gan(self, inputs, targets):\n",
    "        return mean((inputs - targets) ** 2)\n",
    "    \n",
    "    def _age_criterion(self, input, gt_age):\n",
    "        #age_logit, group_logit = self.age_classifier(input)\n",
    "        age_logit = self.age_classifier(input)\n",
    "        return F.mse_loss(self._get_dex_age(age_logit), gt_age)# + \\\n",
    "               #F.cross_entropy(group_logit, age2group(gt_age, self.age_group).long())\n",
    "    \n",
    "    def _get_dex_age(self, pred):\n",
    "        pred = F.softmax(pred, dim=1)\n",
    "        value = tsum(pred * arange(pred.size(1)).to(pred.device), dim=1)\n",
    "        return value\n",
    "    \n",
    "    def _compute_ssim_loss(self, img1, img2, window_size=11):\n",
    "        channel = img1.size(1)\n",
    "        window = self._create_window(window_size, channel).to(img1.device)\n",
    "\n",
    "        mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n",
    "        mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n",
    "\n",
    "        mu1_sq = mu1.pow(2)\n",
    "        mu2_sq = mu2.pow(2)\n",
    "        mu1_mu2 = mu1 * mu2\n",
    "\n",
    "        sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n",
    "        sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n",
    "        sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2\n",
    "\n",
    "        C1 = 0.01 ** 2\n",
    "        C2 = 0.03 ** 2\n",
    "\n",
    "        ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "        return 1.0 - ssim_map.mean()\n",
    "\n",
    "    def _create_window(self, window_size, channel):\n",
    "        _1D_window = self._gaussian(window_size, 1.5).unsqueeze(1)\n",
    "        _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "        window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "        return window\n",
    "    \n",
    "    def _gaussian(self, window_size, sigma):\n",
    "        gauss = Tensor([math.exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])\n",
    "        return gauss / gauss.sum()\n",
    "    \n",
    "    def _extract_vgg_face(self, inputs):\n",
    "        inputs = self._normalize((F.hardtanh(inputs) * 0.5 + 0.5) * 255,\n",
    "                           [129.1863, 104.7624, 93.5940],\n",
    "                           [1.0, 1.0, 1.0])\n",
    "        return self.vgg_face(inputs)\n",
    "    \n",
    "    def _normalize(self, input, mean, std):\n",
    "        mean = Tensor(mean).to(input.device)\n",
    "        std = Tensor(std).to(input.device)\n",
    "        return input.sub(mean[None, :, None, None]).div(std[None, :, None, None])\n",
    "\n",
    "    def _init_model(self):\n",
    "        self.generator = Generator(self.age_group)\n",
    "        self.generator.apply(self._weights_init)\n",
    "\n",
    "        self.discriminator = Discriminator(\n",
    "            age_group=self.age_group,\n",
    "            repeat_num=int(np.log2(self.image_size) - 4),\n",
    "\n",
    "        )\n",
    "\n",
    "        self.vgg_face = torchvision.models.vgg16(num_classes=2622)\n",
    "        self.vgg_face.eval()\n",
    "\n",
    "        self.age_classifier = AuxiliaryAgeClassifier()\n",
    "        self.age_classifier.load_state_dict(\n",
    "            self._load_network(\n",
    "                os.path.join(\n",
    "                    'materials',\n",
    "                    'models',\n",
    "                    'age_sd.pth'\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.age_classifier.eval()\n",
    "\n",
    "        self.optimizer_discriminator = Adam(\n",
    "            self.discriminator.parameters(),\n",
    "            self.init_lr,\n",
    "            betas=(0.5, 0.99)\n",
    "        )\n",
    "\n",
    "        self.optimizer_generator = Adam(\n",
    "            self.generator.parameters(),\n",
    "            self.init_lr,\n",
    "            betas=(0.5, 0.99)\n",
    "        )\n",
    "\n",
    "        self.generator, self.optimizer_generator = self._to_ddp(self.generator, self.optimizer_generator)\n",
    "        self.discriminator, self.optimizer_discriminator = self._to_ddp(self.discriminator, self.optimizer_discriminator)\n",
    "        self.vgg_face = self._to_ddp(self.vgg_face)\n",
    "        self.age_classifier = self._to_ddp(self.age_classifier)\n",
    "    \n",
    "    def _to_ddp(self, modules: Union[list, nn.Module], optimizer: torch.optim.Optimizer = None, opt_level: int = 0) -> Union[DistributedDataParallel, tuple]:\n",
    "        if isinstance(modules, list):\n",
    "            modules = [x.cuda() for x in modules]\n",
    "        else:\n",
    "            modules = modules.cuda()\n",
    "\n",
    "        if optimizer is not None:\n",
    "            modules, optimizer = amp.initialize(\n",
    "                modules,\n",
    "                optimizer,\n",
    "                opt_level=\"O{}\".format(opt_level), verbosity=1\n",
    "            )\n",
    "        \n",
    "        #if isinstance(modules, list):\n",
    "        #    modules = [DistributedDataParallel(x, delay_allreduce=True) for x in modules]\n",
    "        #else:\n",
    "        #    modules = DistributedDataParallel(modules, delay_allreduce=True)\n",
    "        \n",
    "        if optimizer is not None:\n",
    "            return modules, optimizer\n",
    "        else:\n",
    "            return modules\n",
    "\n",
    "    def _load_network(self, state_dict):\n",
    "        if isinstance(state_dict, str):\n",
    "            state_dict = load(state_dict, map_location=device)\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            namekey = k.replace('module.', '')  # remove `module.`\n",
    "            new_state_dict[namekey] = v\n",
    "        return new_state_dict\n",
    "\n",
    "    def _get_train_loader(self):\n",
    "        transforms = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize(self.pretrained_image_size),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        train_dataset = CACD_Dataset(\n",
    "            'materials',\n",
    "            'cacd'\n",
    "        )\n",
    "\n",
    "        #train_sampler = DistributedSampler(\n",
    "        #    train_dataset, shuffle=False\n",
    "        #)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            drop_last=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            #sampler=train_sampler\n",
    "        )\n",
    "\n",
    "        return DataPrefetcher(train_loader, [0, 1])\n",
    "\n",
    "    def _weights_init(self, m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            # torch.nn.init.kaiming_normal(m.weight.data, mode='fan_in')\n",
    "            # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            # m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            if hasattr(m.bias, 'data'):\n",
    "                m.bias.data.fill_(0)\n",
    "        elif classname.find('BatchNorm') != -1:\n",
    "            m.weight.data.normal_(1.0, 0.02)\n",
    "            m.bias.data.fill_(0)\n",
    "        elif classname.find('Linear') != -1:\n",
    "            m.bias.data.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\julia\\.virtualenvs\\ai-b-ai-seminar-gpu\\Lib\\site-packages\\apex\\__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
      "  warnings.warn(msg, DeprecatedFeatureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n"
     ]
    }
   ],
   "source": [
    "pfa_gan = PFA_GAN(\n",
    "    alpha=1e-4,\n",
    "    pix_loss_weight=1e-4,\n",
    "    gan_loss_weight=1e-4,\n",
    "    id_loss_weight=1e-4,\n",
    "    age_loss_weight=1e-4,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 242.00 MiB (GPU 0; 8.00 GiB total capacity; 6.86 GiB already allocated; 0 bytes free; 7.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pfa_gan\u001b[39m.\u001b[39;49mfit()\n",
      "Cell \u001b[1;32mIn[12], line 44\u001b[0m, in \u001b[0;36mPFA_GAN.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mfor\u001b[39;00m n_iter \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestore_iter \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m     43\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefetcher\u001b[39m.\u001b[39mnext()\n\u001b[1;32m---> 44\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(inputs, n_iter\u001b[39m=\u001b[39;49mn_iter)\n\u001b[0;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m n_iter \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_iter \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m n_iter \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter:\n\u001b[0;32m     46\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 116\u001b[0m, in \u001b[0;36mPFA_GAN.train\u001b[1;34m(self, inputs, n_iter)\u001b[0m\n\u001b[0;32m    109\u001b[0m ssim_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_ssim_loss(g_source_pretrained, source_img, window_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m    111\u001b[0m \u001b[39m###########################\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[39m# ID Loss\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[39m###########################\u001b[39;00m\n\u001b[0;32m    114\u001b[0m id_loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmse_loss(\n\u001b[0;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extract_vgg_face(g_source_pretrained),\n\u001b[1;32m--> 116\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_vgg_face(source_img)\n\u001b[0;32m    117\u001b[0m )\n\u001b[0;32m    119\u001b[0m pix_loss_weight \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\n\u001b[0;32m    120\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpix_loss_weight,\n\u001b[0;32m    121\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpix_loss_weight \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecay_pix_factor \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m (n_iter \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecay_pix_n))\n\u001b[0;32m    122\u001b[0m )\n\u001b[0;32m    124\u001b[0m total_loss \u001b[39m=\u001b[39m \\\n\u001b[0;32m    125\u001b[0m     g_loss \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgan_loss_weight \u001b[39m+\u001b[39m \\\n\u001b[0;32m    126\u001b[0m     (l1_loss \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha) \u001b[39m+\u001b[39m ssim_loss \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha) \u001b[39m*\u001b[39m pix_loss_weight \u001b[39m+\u001b[39m \\\n\u001b[0;32m    127\u001b[0m     id_loss \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid_loss_weight \u001b[39m+\u001b[39m \\\n\u001b[0;32m    128\u001b[0m     age_loss \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mage_loss_weight\n",
      "Cell \u001b[1;32mIn[12], line 185\u001b[0m, in \u001b[0;36mPFA_GAN._extract_vgg_face\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_extract_vgg_face\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[0;32m    182\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_normalize((F\u001b[39m.\u001b[39mhardtanh(inputs) \u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m+\u001b[39m \u001b[39m0.5\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m255\u001b[39m,\n\u001b[0;32m    183\u001b[0m                        [\u001b[39m129.1863\u001b[39m, \u001b[39m104.7624\u001b[39m, \u001b[39m93.5940\u001b[39m],\n\u001b[0;32m    184\u001b[0m                        [\u001b[39m1.0\u001b[39m, \u001b[39m1.0\u001b[39m, \u001b[39m1.0\u001b[39m])\n\u001b[1;32m--> 185\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvgg_face(inputs)\n",
      "File \u001b[1;32mc:\\Users\\julia\\.virtualenvs\\ai-b-ai-seminar-gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\julia\\.virtualenvs\\ai-b-ai-seminar-gpu\\Lib\\site-packages\\torchvision\\models\\vgg.py:66\u001b[0m, in \u001b[0;36mVGG.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m---> 66\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[0;32m     67\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[0;32m     68\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\julia\\.virtualenvs\\ai-b-ai-seminar-gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\julia\\.virtualenvs\\ai-b-ai-seminar-gpu\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\julia\\.virtualenvs\\ai-b-ai-seminar-gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\julia\\.virtualenvs\\ai-b-ai-seminar-gpu\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\julia\\.virtualenvs\\ai-b-ai-seminar-gpu\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 242.00 MiB (GPU 0; 8.00 GiB total capacity; 6.86 GiB already allocated; 0 bytes free; 7.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "pfa_gan.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-b-ai-seminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
