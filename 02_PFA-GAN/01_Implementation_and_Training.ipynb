{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PFA-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor, zeros, zeros_like, cat, eye, mean, cuda\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.datasets.folder import pil_loader\n",
    "\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "\n",
    "from apex import amp\n",
    "from apex.parallel import DistributedDataParallel\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "import os\n",
    "import math\n",
    "import h5py\n",
    "import torch\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age2group(age, age_group):\n",
    "    if isinstance(age, np.ndarray):\n",
    "        groups = np.zeros_like(age)\n",
    "    else:\n",
    "        groups = zeros_like(age).to(age.device)\n",
    "\n",
    "    if age_group == 4:\n",
    "        section = [30, 40, 50]\n",
    "    elif age_group == 5:\n",
    "        section = [20, 30, 40, 50]\n",
    "    elif age_group == 7:\n",
    "        section = [10, 20, 30, 40, 50, 60]\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    for i, thresh in enumerate(section, 1):\n",
    "        groups[age > thresh] = i\n",
    "        \n",
    "    return groups[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CACD_Dataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            root_dir='data',\n",
    "            dataset_name='cacd',\n",
    "            age_group=4,\n",
    "            train=False,\n",
    "            source=0,\n",
    "            max_iter=200000,\n",
    "            batch_size=64,\n",
    "            transforms=None,\n",
    "            n_images=-1\n",
    "    ):\n",
    "        self.root_dir = root_dir\n",
    "        self.dataset_name = dataset_name\n",
    "        self.age_group = age_group\n",
    "        self.train = train\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.total_pairs = batch_size * max_iter\n",
    "        self.transforms = transforms\n",
    "        self.n_images = n_images\n",
    "\n",
    "        self._load_meta_data()\n",
    "\n",
    "        self.mean_ages = np.array(\n",
    "            [np.mean(self.ages[self.age_groups == i])\n",
    "            for i in range(self.age_group)]\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        self.label_group_images = []\n",
    "        self.label_group_ages = []\n",
    "\n",
    "        for i in range(self.age_group):\n",
    "            self.label_group_images.append(\n",
    "                self.image_names[self.age_groups == i].tolist())\n",
    "            self.label_group_ages.append(\n",
    "                self.ages[self.age_groups == i].astype(np.float32).tolist())\n",
    "\n",
    "        self.target_labels = np.random.randint(source + 1, self.age_group, self.total_pairs)\n",
    "\n",
    "        pairs = np.array(list(itertools.combinations(range(age_group), 2)))\n",
    "        p = [1, 1, 1, 0.5, 0.5, 0.5]\n",
    "        p = np.array(p) / np.sum(p)\n",
    "        pairs = pairs[np.random.choice(range(len(pairs)), self.total_pairs, p=p), :]\n",
    "        source_labels, target_labels = pairs[:, 0], pairs[:, 1]\n",
    "        self.source_labels = source_labels\n",
    "        self.target_labels = target_labels\n",
    "\n",
    "        self.true_labels = np.random.randint(0, self.age_group, self.total_pairs)\n",
    "\n",
    "    def _load_meta_data(self):\n",
    "        meta = h5py.File(os.path.join(self.root_dir, f\"{self.dataset_name}.mat\"), 'r')\n",
    "\n",
    "        self.ages = meta['celebrityImageData']['age'][0,:]\n",
    "        self.age_groups = np.asanyarray([age2group(np.asanyarray([age]), self.age_group) for age in self.ages])\n",
    "        self.image_names = np.asanyarray(\n",
    "            [''.join(chr(i[0])\n",
    "                    for i in hdf5_object)\n",
    "                    for hdf5_object in [meta[hdf5_object_reference][:] \n",
    "                    for hdf5_object_references in meta['celebrityImageData']['name']\n",
    "                    for hdf5_object_reference in hdf5_object_references]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        if len(self.ages) == len(self.image_names) and len(self.image_names) == len(self.age_groups):\n",
    "            return len(self.ages)\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source_label = self.source_labels[idx]\n",
    "        target_label = self.target_labels[idx]\n",
    "        true_label = self.true_labels[idx]\n",
    "\n",
    "        source_img = transforms.ToTensor()(pil_loader(os.path.join(self.root_dir, self.dataset_name, random.choice(self.label_group_images[source_label]))).resize((248,248)))\n",
    "\n",
    "        index = random.randint(0, len(self.label_group_images[true_label]) - 1)\n",
    "        true_img = transforms.ToTensor()(pil_loader(os.path.join(self.root_dir, self.dataset_name, self.label_group_images[true_label][index])).resize((248,248)))\n",
    "        true_age = self.label_group_ages[true_label][index]\n",
    "        mean_age = self.mean_ages[target_label]\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            source_img = self.transforms(source_img)\n",
    "            true_img = self.transforms(true_img)\n",
    "\n",
    "        return source_img, true_img, source_label, target_label, true_label, true_age, mean_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPrefetcher():\n",
    "    def __init__(self, loader, *norm_index):\n",
    "        self.loader = iter(loader)\n",
    "        self.normlize = lambda x: x.sub_(0.5).div_(0.5)\n",
    "        self.norm_index = norm_index\n",
    "        self.stream = cuda.Stream()\n",
    "        self.preload()\n",
    "\n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_input = next(self.loader)\n",
    "        except StopIteration:\n",
    "            self.next_input = None\n",
    "            return\n",
    "        with cuda.stream(self.stream):\n",
    "            self.next_input = [\n",
    "                self.normlize(x.cuda(non_blocking=True)) if i in self.norm_index else x.cuda(non_blocking=True)\n",
    "                for i, x in enumerate(self.next_input)\n",
    "            ]\n",
    "\n",
    "    def next(self):\n",
    "        cuda.current_stream().wait_stream(self.stream)\n",
    "        input = self.next_input\n",
    "        self.preload()\n",
    "        return input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PFA-GAN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, age_group, conv_dim=64, repeat_num=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.age_group = age_group\n",
    "        self.conv_dim = conv_dim\n",
    "        self.repeat_num = repeat_num\n",
    "        \n",
    "        self._init_model()\n",
    "\n",
    "    def _init_model(self):\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            3,\n",
    "            self.conv_dim,\n",
    "            kernel_size=4,\n",
    "            stride=2,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "        layers = []\n",
    "        nf_mult = 1\n",
    "\n",
    "        # gradually increase the number of filters\n",
    "        for n in range(1, self.repeat_num):\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** n, 8)\n",
    "            \n",
    "            layers += [\n",
    "                nn.utils.spectral_norm(\n",
    "                    nn.Conv2d(\n",
    "                        in_channels=self.conv_dim * nf_mult_prev + (self.age_group if n == 1 else 0),\n",
    "                        out_channels=self.conv_dim * nf_mult,\n",
    "                        kernel_size=4,\n",
    "                        stride=2,\n",
    "                        padding=1,\n",
    "                        bias=True\n",
    "                    )\n",
    "                ),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "        \n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** self.repeat_num, 8)\n",
    "        \n",
    "        layers += [\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.conv_dim * nf_mult_prev,\n",
    "                out_channels=self.conv_dim * nf_mult,\n",
    "                kernel_size=4,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(self.conv_dim * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.conv_dim * nf_mult,\n",
    "                out_channels=1,\n",
    "                kernel_size=4,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        self.main = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, inputs, condition):\n",
    "        x = F.leaky_relu(self.conv1(inputs), 0.2, inplace=True)\n",
    "        condition = self._group2feature(\n",
    "            condition.to(device),\n",
    "            feature_size=x.size(2),\n",
    "            age_group=self.age_group\n",
    "        ).to(x)\n",
    "        return self.main(cat([x, condition], dim=1))\n",
    "\n",
    "    def _group2feature(self, group, age_group, feature_size):\n",
    "        onehot = self._group2onehot(\n",
    "            group, \n",
    "            age_group\n",
    "        )\n",
    "        return onehot.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, feature_size, feature_size).to(device)\n",
    "\n",
    "    def _group2onehot(self, groups, age_group):\n",
    "        code = eye(age_group).to(device)[groups.squeeze()]\n",
    "        if len(code.size()) > 1:\n",
    "            return code\n",
    "        return code.unsqueeze(0).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResidualBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.n_channels = channels\n",
    "        self._init_model()\n",
    "    \n",
    "    def _init_model(self):\n",
    "        layers = [\n",
    "            nn.Conv2d(\n",
    "                self.n_channels,\n",
    "                self.n_channels,\n",
    "                3,\n",
    "                1,\n",
    "                1\n",
    "            ),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(\n",
    "                self.n_channels,\n",
    "                self.n_channels,\n",
    "                3,\n",
    "                1,\n",
    "                1\n",
    "            ),\n",
    "            nn.BatchNorm2d(self.n_channels),\n",
    "        ]\n",
    "\n",
    "        self.main = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.main(x)\n",
    "        return F.leaky_relu(residual + x, 0.2, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorSubNetwork(nn.Module):\n",
    "    def __init__(self, in_channels=3, n_residual_blocks=4):\n",
    "        super(GeneratorSubNetwork, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.n_residual_blocks = n_residual_blocks\n",
    "\n",
    "        self._init_model()\n",
    "    \n",
    "    def _init_model(self):\n",
    "        layers = [\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.in_channels,\n",
    "                out_channels=32,\n",
    "                kernel_size=9,\n",
    "                stride=1,\n",
    "                padding=4\n",
    "            ),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "\n",
    "        for _ in range(self.n_residual_blocks):\n",
    "            layers.append(ResidualBlock(128))\n",
    "\n",
    "        layers.extend([\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=128,\n",
    "                out_channels=64,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=64,\n",
    "                out_channels=32,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=3,\n",
    "                kernel_size=9,\n",
    "                stride=1,\n",
    "                padding=4\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "        self.main = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, age_group, n_residual_blocks=4):\n",
    "        super(Generator, self).__init__()\n",
    "        self.age_group = age_group\n",
    "        self._init_model(n_residual_blocks)\n",
    "    \n",
    "    def _init_model(self, n_residual_blocks):\n",
    "        self.sub_networks = nn.ModuleList()\n",
    "\n",
    "        for _ in range(self.age_group - 1):\n",
    "            self.sub_networks.append(GeneratorSubNetwork(\n",
    "                in_channels=3,\n",
    "                n_residual_blocks=n_residual_blocks\n",
    "            ))\n",
    "    \n",
    "    def forward(self, x, source_label: Tensor, target_label: Tensor):\n",
    "        condition = self._pfa_encoding(source_label, target_label, self.age_group).to(device)\n",
    "        for i in range(self.age_group - 1):\n",
    "            aging_effects = self.sub_networks[i](x)\n",
    "            x = x + aging_effects * condition[:, i]\n",
    "        return x\n",
    "\n",
    "    def _pfa_encoding(self, source, target, age_group):\n",
    "        source, target = source.long(), target.long()\n",
    "        code = zeros((source.size(0), age_group - 1, 1, 1, 1)).to(source)\n",
    "        for i in range(source.size(0)):\n",
    "            code[i, source[i]: target[i], ...] = 1\n",
    "        return code\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxiliaryAgeClassifier(nn.Module):\n",
    "    def __init__(self, pool='max'):\n",
    "        super(AuxiliaryAgeClassifier, self).__init__()\n",
    "        \n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.fc6 = nn.Linear(25088, 4096, bias=True)\n",
    "        self.fc7 = nn.Linear(4096, 4096, bias=True)\n",
    "        self.fc8_101 = nn.Linear(4096, 101, bias=True)\n",
    "        \n",
    "        if pool == 'max':\n",
    "            self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        elif pool == 'avg':\n",
    "            self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool4 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool5 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = {}\n",
    "        out['r11'] = F.relu(self.conv1_1(x))\n",
    "        out['r12'] = F.relu(self.conv1_2(out['r11']))\n",
    "        out['p1'] = self.pool1(out['r12'])\n",
    "        out['r21'] = F.relu(self.conv2_1(out['p1']))\n",
    "        out['r22'] = F.relu(self.conv2_2(out['r21']))\n",
    "        out['p2'] = self.pool2(out['r22'])\n",
    "        out['r31'] = F.relu(self.conv3_1(out['p2']))\n",
    "        out['r32'] = F.relu(self.conv3_2(out['r31']))\n",
    "        out['r33'] = F.relu(self.conv3_3(out['r32']))\n",
    "        out['p3'] = self.pool3(out['r33'])\n",
    "        out['r41'] = F.relu(self.conv4_1(out['p3']))\n",
    "        out['r42'] = F.relu(self.conv4_2(out['r41']))\n",
    "        out['r43'] = F.relu(self.conv4_3(out['r42']))\n",
    "        out['p4'] = self.pool4(out['r43'])\n",
    "        out['r51'] = F.relu(self.conv5_1(out['p4']))\n",
    "        out['r52'] = F.relu(self.conv5_2(out['r51']))\n",
    "        out['r53'] = F.relu(self.conv5_3(out['r52']))\n",
    "        out['p5'] = self.pool5(out['r53'])\n",
    "        out['p5'] = out['p5'].view(out['p5'].size(0), -1)\n",
    "        out['fc6'] = F.relu(self.fc6(out['p5']))\n",
    "        out['fc7'] = F.relu(self.fc7(out['fc6']))\n",
    "        out['fc8'] = self.fc8_101(out['fc7'])\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PFA-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PFA_GAN(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            alpha_fea,\n",
    "            alpha_ssim,\n",
    "            pix_loss_weight,\n",
    "            gan_loss_weight,\n",
    "            id_loss_weight,\n",
    "            age_loss_weight,\n",
    "            age_group=4,\n",
    "            init_lr=1e-4,\n",
    "            decay_pix_factor=0.,\n",
    "            decay_pix_n=2000,\n",
    "            batch_size=64,\n",
    "            image_size=256,\n",
    "            print_log=50,\n",
    "            save_iter=10,\n",
    "            restore_iter=0,\n",
    "            max_iter=200000,\n",
    "            num_workers=0,\n",
    "            experiment=0):\n",
    "        \n",
    "        self.age_group = age_group\n",
    "        self.image_size = image_size\n",
    "        self.init_lr = init_lr\n",
    "        self.print_log = print_log\n",
    "        self.save_iter = save_iter\n",
    "        self.restore_iter = restore_iter\n",
    "        self.max_iter = max_iter\n",
    "        self.pix_loss_weight = pix_loss_weight\n",
    "        self.decay_pix_factor = decay_pix_factor\n",
    "        self.decay_pix_n = decay_pix_n\n",
    "        self.gan_loss_weight = gan_loss_weight\n",
    "        self.alpha_fea = alpha_fea\n",
    "        self.alpha_ssim = alpha_ssim\n",
    "        self.id_loss_weight = id_loss_weight\n",
    "        self.age_loss_weight = age_loss_weight\n",
    "        self.num_workers = num_workers\n",
    "        self.batch_size = batch_size\n",
    "        self.experiment = experiment\n",
    "\n",
    "        self.test_images = self._get_test_images()\n",
    "        self.prefetcher = self._get_train_loader()\n",
    "\n",
    "        self._init_model()\n",
    "\n",
    "        self.generator_losses = []\n",
    "        self.discriminator_losses = []\n",
    "        self.age_losses = []\n",
    "        self.l1_losses = []\n",
    "        self.ssim_losses = []\n",
    "        self.id_losses = []\n",
    "        self.total_losses = []\n",
    "    \n",
    "    def fit(self):\n",
    "        for n_iter in range(self.restore_iter + 1, self.max_iter + 1):\n",
    "            inputs = self.prefetcher.next()\n",
    "\n",
    "            d1_logit, d3_logit, g_loss, d_loss, gan_logit, age_loss, l1_loss, ssim_loss, id_loss, total_loss = self.train(inputs, n_iter=n_iter)\n",
    "\n",
    "            self.generator_losses.append(g_loss)\n",
    "            self.discriminator_losses.append(d_loss)\n",
    "            self.age_losses.append(age_loss)\n",
    "            self.l1_losses.append(l1_loss)\n",
    "            self.ssim_losses.append(ssim_loss)\n",
    "            self.id_losses.append(id_loss)\n",
    "            self.total_losses.append(total_loss)\n",
    "\n",
    "            if d1_logit is None:\n",
    "                self.prefetcher = self._get_train_loader()\n",
    "\n",
    "            if n_iter % self.print_log == 0:\n",
    "                print(f'[{n_iter}/{self.max_iter + 1}]\\tG: {g_loss}\\tD: {d_loss}\\tAge: {age_loss}\\tL1: {l1_loss}\\tSSIMM: {ssim_loss}\\tID: {id_loss}\\tTotal: {total_loss}')\n",
    "            \n",
    "            if n_iter % self.save_iter == 0 or n_iter == self.max_iter:\n",
    "                self._save_model(os.path.join('model', 'pfa-gan', 'checkpoint', f'exp_{self.experiment}'), n_epoch=n_iter, is_checkpoint=True)\n",
    "                self.generate_images(n_iter)\n",
    "        \n",
    "        self._save_model(os.path.join('model', 'pfa-gan'), is_checkpoint=False)\n",
    "\n",
    "    def train(self, inputs, n_iter):\n",
    "        try:\n",
    "            source_img, true_img, source_label, target_label, true_label, true_age, mean_age = inputs\n",
    "            source_img, true_img, source_label, target_label, true_label, true_age, mean_age = source_img.to(device), true_img.to(device), source_label.to(device), target_label.to(device), true_label.to(device), true_age.to(device), mean_age.to(device)\n",
    "        except:\n",
    "            return [None, None, None, None, None, None, None, None, None, None]\n",
    "        \n",
    "        self.generator.train()\n",
    "        self.discriminator.train()\n",
    "        \n",
    "        g_source = self.generator(source_img, source_label, target_label)\n",
    "\n",
    "        ###########################\n",
    "        # Train Discriminator\n",
    "        ###########################\n",
    "        self.discriminator_optimizer.zero_grad()\n",
    "\n",
    "        d1_logit = self.discriminator(true_img, true_label)\n",
    "        d3_logit = self.discriminator(g_source.detach(), target_label)\n",
    "\n",
    "        d_loss = 0.5 * (self._ls_gan(d1_logit, 1.) + self._ls_gan(d3_logit, 0.))\n",
    "\n",
    "        with amp.scale_loss(d_loss, self.discriminator_optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "\n",
    "        self.discriminator_optimizer.step()\n",
    "\n",
    "        ###########################\n",
    "        # Train Generator\n",
    "        ###########################\n",
    "        self.generator_optimizer.zero_grad()\n",
    "\n",
    "        ###########################\n",
    "        # GAN Loss\n",
    "        ###########################\n",
    "        gan_logit = self.discriminator(g_source, target_label)\n",
    "        g_loss = self._ls_gan(gan_logit, 1.)\n",
    "\n",
    "        ###########################\n",
    "        # Age Loss\n",
    "        ###########################\n",
    "        age_loss = self._age_criterion(g_source, mean_age)\n",
    "\n",
    "        ###########################\n",
    "        # L1 Loss\n",
    "        ###########################\n",
    "        l1_loss = F.l1_loss(g_source, source_img)\n",
    "\n",
    "        ###########################\n",
    "        # SSIM Loss\n",
    "        ###########################\n",
    "        ssim_loss = self._compute_ssim_loss(g_source, source_img)\n",
    "\n",
    "        ###########################\n",
    "        # ID Loss\n",
    "        ###########################\n",
    "        id_loss = F.mse_loss(\n",
    "            self._extract_vgg_face(g_source),\n",
    "            self._extract_vgg_face(source_img)\n",
    "        )\n",
    "\n",
    "        ###########################\n",
    "        # Pixel-Wise Loss\n",
    "        ###########################\n",
    "        pix_loss_weight = max(\n",
    "            self.pix_loss_weight,\n",
    "            self.pix_loss_weight * (self.decay_pix_factor ** (n_iter // self.decay_pix_n))\n",
    "        )\n",
    "\n",
    "        ###########################\n",
    "        # Total Loss\n",
    "        ###########################\n",
    "        total_loss = \\\n",
    "            g_loss * self.gan_loss_weight + \\\n",
    "            age_loss * self.age_loss_weight + \\\n",
    "            id_loss * self.id_loss_weight + \\\n",
    "            (l1_loss * (1 - self.alpha_fea) + ssim_loss * self.alpha_ssim) * pix_loss_weight\n",
    "        \n",
    "        with amp.scale_loss(total_loss, self.generator_optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        self.generator_optimizer.step()\n",
    "\n",
    "        return [\n",
    "            d1_logit,\n",
    "            d3_logit,\n",
    "            d_loss,\n",
    "            g_loss,\n",
    "            gan_logit,\n",
    "            age_loss,\n",
    "            l1_loss,\n",
    "            ssim_loss,\n",
    "            id_loss,\n",
    "            total_loss\n",
    "        ]\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_images(self, n_iter, n_source=0, age_group=4):\n",
    "        self.generator.eval()\n",
    "        \n",
    "        real_img = self.test_images[0].to(device)\n",
    "        bs, ch, w, h = real_img.size()\n",
    "        fake_imgs = [real_img, ]\n",
    "\n",
    "        for target in range(n_source + 1, age_group):\n",
    "            output = self.generator(real_img, torch.ones(bs) * n_source, torch.ones(bs) * target)\n",
    "            fake_imgs.append(output)\n",
    "            \n",
    "        fake_imgs = torch.stack(fake_imgs).transpose(1, 0).reshape((-1, ch, w, h))\n",
    "\n",
    "        fake_imgs = fake_imgs * 0.5 + 0.5\n",
    "        grid_img = torchvision.utils.make_grid(fake_imgs.clamp(0., 1.), nrow=age_group - n_source)\n",
    "\n",
    "        self._save_image(\n",
    "            grid_img,\n",
    "            os.path.join(\n",
    "                'test',\n",
    "                'generations',\n",
    "                f'exp_{self.experiment}'\n",
    "            ),\n",
    "            f'{n_iter}_{self.experiment}_test.jpg'\n",
    "        )\n",
    "    \n",
    "    def _save_image(self, grid_img, path, img_name):\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        \n",
    "        save_image(\n",
    "            grid_img,\n",
    "            os.path.join(path, img_name),\n",
    "            nrow=1\n",
    "        )\n",
    "\n",
    "    def _ls_gan(self, inputs, targets):\n",
    "        return mean((inputs - targets) ** 2)\n",
    "    \n",
    "    def _age_criterion(self, input, gt_age):\n",
    "        age_logit = self._extract_ages(input)\n",
    "        return F.mse_loss(age_logit, gt_age)\n",
    "        \n",
    "    def _extract_ages(self, x):\n",
    "        x = F.interpolate(x, size=(224, 224), mode='bilinear')\n",
    "        predict_age_pb = self.age_classifier(x)['fc8']\n",
    "        predicted_age = self.__get_predicted_age(predict_age_pb)\n",
    "        return predicted_age\n",
    "\n",
    "    def __get_predicted_age(self, age_pb):\n",
    "        predict_age_pb = F.softmax(age_pb, dim=1)\n",
    "        predict_age = torch.zeros(age_pb.size(0)).type_as(predict_age_pb)\n",
    "        for i in range(age_pb.size(0)):\n",
    "            for j in range(age_pb.size(1)):\n",
    "                predict_age[i] += j * predict_age_pb[i][j]\n",
    "        return predict_age\n",
    "    \n",
    "    def _compute_ssim_loss(self, img1, img2, window_size=10):\n",
    "        channel = img1.size(1)\n",
    "        window = self._create_window(window_size, channel).to(img1.device)\n",
    "\n",
    "        mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n",
    "        mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n",
    "\n",
    "        mu1_sq = mu1.pow(2)\n",
    "        mu2_sq = mu2.pow(2)\n",
    "        mu1_mu2 = mu1 * mu2\n",
    "\n",
    "        sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n",
    "        sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n",
    "        sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2\n",
    "\n",
    "        C1 = 0.01 ** 2\n",
    "        C2 = 0.03 ** 2\n",
    "\n",
    "        ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "        return 1.0 - ssim_map.mean()\n",
    "\n",
    "    def _create_window(self, window_size, channel):\n",
    "        _1D_window = self._gaussian(window_size, 1.5).unsqueeze(1)\n",
    "        _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "        window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "        return window\n",
    "    \n",
    "    def _gaussian(self, window_size, sigma):\n",
    "        gauss = Tensor([math.exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])\n",
    "        return gauss / gauss.sum()\n",
    "    \n",
    "    def _extract_vgg_face(self, inputs):\n",
    "        inputs = self._normalize((F.hardtanh(inputs) * 0.5 + 0.5) * 255,\n",
    "                           [129.1863, 104.7624, 93.5940],\n",
    "                           [1.0, 1.0, 1.0])\n",
    "        return self.vgg_face(inputs)\n",
    "    \n",
    "    def _normalize(self, input, mean, std):\n",
    "        mean = Tensor(mean).to(input.device)\n",
    "        std = Tensor(std).to(input.device)\n",
    "        return input.sub(mean[None, :, None, None]).div(std[None, :, None, None])\n",
    "\n",
    "    def _init_model(self):\n",
    "        self.generator = Generator(self.age_group)\n",
    "        self.generator.apply(self._weights_init)\n",
    "\n",
    "        self.discriminator = Discriminator(\n",
    "            age_group=self.age_group,\n",
    "            repeat_num=int(np.log2(self.image_size) - 4),\n",
    "        )\n",
    "\n",
    "        self.vgg_face = InceptionResnetV1(pretrained='vggface2')\n",
    "        self.vgg_face.cuda()\n",
    "        self.vgg_face.eval()\n",
    "\n",
    "        self.age_classifier = AuxiliaryAgeClassifier()\n",
    "        ckpt = torch.load(\n",
    "            os.path.join(\n",
    "                    'models',\n",
    "                    'pre-trained',\n",
    "                    'dex_age_classifier.pth'\n",
    "                ),\n",
    "            map_location=\"cpu\"\n",
    "        )['state_dict']\n",
    "        ckpt = {k.replace('-', '_'): v for k, v in ckpt.items()}\n",
    "        self.age_classifier.load_state_dict(ckpt)\n",
    "        self.age_classifier.cuda() \n",
    "        self.age_classifier.eval()\n",
    "\n",
    "        self.discriminator_optimizer = Adam(\n",
    "            self.discriminator.parameters(),\n",
    "            self.init_lr,\n",
    "            betas=(0.5, 0.99)\n",
    "        )\n",
    "\n",
    "        self.generator_optimizer = Adam(\n",
    "            self.generator.parameters(),\n",
    "            self.init_lr,\n",
    "            betas=(0.5, 0.99)\n",
    "        )\n",
    "\n",
    "        if self.restore_iter > 0:\n",
    "            self._load_model(\n",
    "                os.path.join(\n",
    "                    'model',\n",
    "                    'pfa-gan',\n",
    "                    'checkpoint',\n",
    "                    f'exp_{self.experiment}'\n",
    "                ),\n",
    "                self.restore_iter,\n",
    "                is_checkpoint=True\n",
    "            )\n",
    "\n",
    "        self.generator, self.generator_optimizer = self._to_ddp(self.generator, self.generator_optimizer)\n",
    "        self.discriminator, self.discriminator_optimizer = self._to_ddp(self.discriminator, self.discriminator_optimizer)\n",
    "        self.vgg_face = self._to_ddp(self.vgg_face)\n",
    "        self.age_classifier = self._to_ddp(self.age_classifier)\n",
    "    \n",
    "    def _to_ddp(self, modules: Union[list, nn.Module], optimizer: torch.optim.Optimizer = None, opt_level: int = 0) -> Union[DistributedDataParallel, tuple]:\n",
    "        if isinstance(modules, list):\n",
    "            modules = [x.cuda() for x in modules]\n",
    "        else:\n",
    "            modules = modules.cuda()\n",
    "\n",
    "        if optimizer is not None:\n",
    "            modules, optimizer = amp.initialize(\n",
    "                modules,\n",
    "                optimizer,\n",
    "                opt_level=\"O{}\".format(opt_level), verbosity=1\n",
    "            )\n",
    "            \n",
    "        if optimizer is not None:\n",
    "            return modules, optimizer\n",
    "        else:\n",
    "            return modules\n",
    "\n",
    "    def _get_train_loader(self):\n",
    "        train_dataset = CACD_Dataset(\n",
    "            'data',\n",
    "            'cacd',\n",
    "            age_group=self.age_group\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "        return DataPrefetcher(train_loader, [0, 1])\n",
    "\n",
    "    def _get_test_images(self):\n",
    "        test_dataset = CACD_Dataset(\n",
    "            'data',\n",
    "            'cacd'\n",
    "        )\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            dataset=test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "        return next(iter(test_loader))\n",
    "\n",
    "    def _weights_init(self, m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            if hasattr(m.bias, 'data'):\n",
    "                m.bias.data.fill_(0)\n",
    "        elif classname.find('BatchNorm') != -1:\n",
    "            m.weight.data.normal_(1.0, 0.02)\n",
    "            m.bias.data.fill_(0)\n",
    "        elif classname.find('Linear') != -1:\n",
    "            m.bias.data.zero_()\n",
    "    \n",
    "    def _save_model(self, path, n_epoch=0, is_checkpoint=False):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        \n",
    "        path = os.path.join(path, 'pfa-gan.obj') if not is_checkpoint else os.path.join(path, f'pfa-gan_{n_epoch}.obj')\n",
    "\n",
    "        torch.save({\n",
    "            'discriminator_state_dict': self.discriminator.state_dict(),\n",
    "            'discriminator_optimizer_state_dict': self.discriminator_optimizer.state_dict(),\n",
    "            'generator_state_dict': self.generator.state_dict(),\n",
    "            'generator_optimizer_state_dict': self.generator_optimizer.state_dict()\n",
    "        }, path)\n",
    "    \n",
    "    def _load_model(self, path, n_epoch=0, is_checkpoint=True):\n",
    "        path = os.path.join(path, 'pfa-gan.obj') if not is_checkpoint else os.path.join(path, f'pfa-gan_{n_epoch}.obj')\n",
    "\n",
    "        checkpoint = torch.load(path)\n",
    "\n",
    "        self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        self.discriminator_optimizer.load_state_dict(checkpoint['discriminator_optimizer_state_dict'])\n",
    "        self.generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "        self.generator_optimizer.load_state_dict(checkpoint['generator_optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/js07987/.local/lib/python3.11/site-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
      "  warnings.warn(msg, DeprecatedFeatureWarning)\n"
     ]
    }
   ],
   "source": [
    "pfa_gan = PFA_GAN(\n",
    "    alpha_fea=0.025,\n",
    "    alpha_ssim=0.15,\n",
    "    gan_loss_weight=100,\n",
    "    id_loss_weight=0.2,\n",
    "    age_loss_weight=0.4,\n",
    "    pix_loss_weight=0.,\n",
    "    decay_pix_factor=0.7,\n",
    "    decay_pix_n=15,\n",
    "    batch_size=8,\n",
    "    save_iter=5000,\n",
    "    restore_iter=0,\n",
    "    num_workers=4,\n",
    "    experiment=17\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50/200001]\tG: 0.02471981570124626\tD: 1.081299066543579\tAge: 19.415863037109375\tL1: 0.8416144847869873\tSSIMM: 0.9785171151161194\tID: 0.0011401950614526868\tTotal: 115.89647674560547\n",
      "[100/200001]\tG: 0.007617887109518051\tD: 1.0120173692703247\tAge: 39.716339111328125\tL1: 0.8895083665847778\tSSIMM: 0.9803962111473083\tID: 0.0017684490885585546\tTotal: 117.088623046875\n",
      "[150/200001]\tG: 0.07804854959249496\tD: 1.2323099374771118\tAge: 26.370214462280273\tL1: 0.5911006927490234\tSSIMM: 0.9701908826828003\tID: 0.0017123543657362461\tTotal: 133.7794189453125\n",
      "[200/200001]\tG: 0.007154666818678379\tD: 0.9038313627243042\tAge: 22.186792373657227\tL1: 0.4404042661190033\tSSIMM: 0.9512966275215149\tID: 0.0015032407827675343\tTotal: 99.25814819335938\n",
      "[250/200001]\tG: 0.012997382320463657\tD: 0.9455162286758423\tAge: 45.4011116027832\tL1: 0.3111651539802551\tSSIMM: 0.9413520097732544\tID: 0.0010896653402596712\tTotal: 112.71228790283203\n",
      "[300/200001]\tG: 0.006615258287638426\tD: 1.0604276657104492\tAge: 12.15858268737793\tL1: 0.39306047558784485\tSSIMM: 0.9330573081970215\tID: 0.001491266069933772\tTotal: 110.90650177001953\n",
      "[350/200001]\tG: 0.010013308376073837\tD: 1.0545679330825806\tAge: 11.153635025024414\tL1: 0.25310277938842773\tSSIMM: 0.8850648999214172\tID: 0.0005425439449027181\tTotal: 109.9183578491211\n",
      "[400/200001]\tG: 0.05818943679332733\tD: 1.1099839210510254\tAge: 28.373035430908203\tL1: 0.2515903115272522\tSSIMM: 0.901019811630249\tID: 0.0007901388453319669\tTotal: 122.34776306152344\n",
      "[450/200001]\tG: 0.00099108275026083\tD: 0.9919877052307129\tAge: 26.776464462280273\tL1: 0.34164756536483765\tSSIMM: 0.9323952794075012\tID: 0.0011933119967579842\tTotal: 109.90959167480469\n",
      "[500/200001]\tG: 0.0017820291686803102\tD: 1.0175498723983765\tAge: 18.621906280517578\tL1: 0.3474089205265045\tSSIMM: 0.9274982213973999\tID: 0.0014256981667131186\tTotal: 109.20403289794922\n",
      "[550/200001]\tG: 0.004495361819863319\tD: 0.9430093169212341\tAge: 19.339269638061523\tL1: 0.3671536147594452\tSSIMM: 0.9235507249832153\tID: 0.0017971489578485489\tTotal: 102.03700256347656\n",
      "[600/200001]\tG: 0.004986914806067944\tD: 0.9964241981506348\tAge: 8.001212120056152\tL1: 0.5280352830886841\tSSIMM: 0.9489887356758118\tID: 0.0021328148432075977\tTotal: 102.84333038330078\n",
      "[650/200001]\tG: 0.004655828699469566\tD: 0.9290029406547546\tAge: 10.211194038391113\tL1: 0.25649702548980713\tSSIMM: 0.8930132389068604\tID: 0.0012001919094473124\tTotal: 96.9850082397461\n",
      "[700/200001]\tG: 0.0011721678311005235\tD: 0.9994374513626099\tAge: 13.990156173706055\tL1: 0.26383039355278015\tSSIMM: 0.9091107845306396\tID: 0.0005762673099525273\tTotal: 105.53992462158203\n",
      "[750/200001]\tG: 0.0008085680892691016\tD: 0.9905691146850586\tAge: 9.73786735534668\tL1: 0.2716360092163086\tSSIMM: 0.8848199248313904\tID: 0.0010367510840296745\tTotal: 102.9522705078125\n",
      "[800/200001]\tG: 0.010692295618355274\tD: 0.9714981913566589\tAge: 4.965630054473877\tL1: 0.3341725766658783\tSSIMM: 0.9064049124717712\tID: 0.002067822264507413\tTotal: 99.13648223876953\n",
      "[850/200001]\tG: 0.010386811569333076\tD: 0.865319550037384\tAge: 26.206134796142578\tL1: 0.23132798075675964\tSSIMM: 0.8894127011299133\tID: 0.0006380669074133039\tTotal: 97.01453399658203\n",
      "[900/200001]\tG: 0.0007587310974486172\tD: 1.0172498226165771\tAge: 8.38178825378418\tL1: 0.35594695806503296\tSSIMM: 0.9353442192077637\tID: 0.001143518602475524\tTotal: 105.07792663574219\n",
      "[950/200001]\tG: 0.0031832915265113115\tD: 1.1053398847579956\tAge: 42.21379852294922\tL1: 0.24644938111305237\tSSIMM: 0.8622291088104248\tID: 0.0008681295439600945\tTotal: 127.41968536376953\n",
      "[1000/200001]\tG: 0.0012736509088426828\tD: 1.00857412815094\tAge: 21.64072036743164\tL1: 0.24523773789405823\tSSIMM: 0.9211809039115906\tID: 0.0009845799067988992\tTotal: 109.51390075683594\n",
      "[1050/200001]\tG: 0.03025789186358452\tD: 0.9369171857833862\tAge: 16.65297508239746\tL1: 0.2926178574562073\tSSIMM: 0.9314367771148682\tID: 0.0009336908115074039\tTotal: 100.35308837890625\n",
      "[1100/200001]\tG: 0.0006846395554021001\tD: 1.0355045795440674\tAge: 2.1420204639434814\tL1: 0.34686458110809326\tSSIMM: 0.947624921798706\tID: 0.0010857812594622374\tTotal: 104.40748596191406\n",
      "[1150/200001]\tG: 0.003006397280842066\tD: 0.9745214581489563\tAge: 2.5599584579467773\tL1: 0.23849798738956451\tSSIMM: 0.9153434634208679\tID: 0.0012716061901301146\tTotal: 98.47638702392578\n",
      "[1200/200001]\tG: 0.0011292097624391317\tD: 1.0304259061813354\tAge: 9.384932518005371\tL1: 0.34011927247047424\tSSIMM: 0.8983773589134216\tID: 0.0013472877908498049\tTotal: 106.79682922363281\n",
      "[1250/200001]\tG: 0.04284803569316864\tD: 0.8225948214530945\tAge: 24.914596557617188\tL1: 0.22658278048038483\tSSIMM: 0.8865411281585693\tID: 0.001298560295253992\tTotal: 92.22557830810547\n",
      "[1300/200001]\tG: 0.0011944586876779795\tD: 1.034705400466919\tAge: 8.155496597290039\tL1: 0.3726615607738495\tSSIMM: 0.9389169216156006\tID: 0.001138008781708777\tTotal: 106.73297119140625\n",
      "[1350/200001]\tG: 0.0010168147273361683\tD: 0.9797529578208923\tAge: 24.08836555480957\tL1: 0.4085442125797272\tSSIMM: 0.9645669460296631\tID: 0.0016479389742016792\tTotal: 107.61096954345703\n",
      "[1400/200001]\tG: 0.0022313015069812536\tD: 0.9627671241760254\tAge: 66.05492401123047\tL1: 0.3141646683216095\tSSIMM: 0.9460902214050293\tID: 0.0016236173687502742\tTotal: 122.69901275634766\n",
      "[1450/200001]\tG: 0.0013339531142264605\tD: 0.9977615475654602\tAge: 6.93395471572876\tL1: 0.29026225209236145\tSSIMM: 0.9023873209953308\tID: 0.001978099811822176\tTotal: 102.55013275146484\n",
      "[1500/200001]\tG: 0.0008834392647258937\tD: 1.0048763751983643\tAge: 16.589622497558594\tL1: 0.4288049638271332\tSSIMM: 0.9229986071586609\tID: 0.0019371429225429893\tTotal: 107.1238784790039\n",
      "[1550/200001]\tG: 0.0005636658170260489\tD: 0.9879146814346313\tAge: 9.957414627075195\tL1: 0.43204888701438904\tSSIMM: 0.9410150051116943\tID: 0.0022949129343032837\tTotal: 102.77488708496094\n",
      "[1600/200001]\tG: 0.017742935568094254\tD: 1.0914067029953003\tAge: 12.706969261169434\tL1: 0.411442369222641\tSSIMM: 0.9253182411193848\tID: 0.0025247600860893726\tTotal: 114.22396087646484\n",
      "[1650/200001]\tG: 0.0011915862560272217\tD: 0.9831516146659851\tAge: 4.276533126831055\tL1: 0.2324286550283432\tSSIMM: 0.8807521462440491\tID: 0.0008040241664275527\tTotal: 100.02593231201172\n",
      "[1700/200001]\tG: 0.0002625651250127703\tD: 1.0052287578582764\tAge: 15.960515975952148\tL1: 0.35558760166168213\tSSIMM: 0.9400073885917664\tID: 0.0014852196909487247\tTotal: 106.90737915039062\n",
      "[1750/200001]\tG: 0.0013102563098073006\tD: 1.0010437965393066\tAge: 24.604530334472656\tL1: 0.380356103181839\tSSIMM: 0.914256751537323\tID: 0.0014336195308715105\tTotal: 109.94647979736328\n",
      "[1800/200001]\tG: 0.0033330353908240795\tD: 1.0409009456634521\tAge: 7.417652130126953\tL1: 0.3258378803730011\tSSIMM: 0.8845229744911194\tID: 0.001526170875877142\tTotal: 107.05746459960938\n",
      "[1850/200001]\tG: 0.0009425552561879158\tD: 0.971653163433075\tAge: 18.62305450439453\tL1: 0.4443783164024353\tSSIMM: 0.9280182123184204\tID: 0.0017555843805894256\tTotal: 104.61488342285156\n",
      "[1900/200001]\tG: 0.0008413000614382327\tD: 0.965225875377655\tAge: 23.68289566040039\tL1: 0.5267229676246643\tSSIMM: 0.9469773173332214\tID: 0.001599023351445794\tTotal: 105.9960708618164\n",
      "[1950/200001]\tG: 0.022354185581207275\tD: 1.07867431640625\tAge: 18.71886444091797\tL1: 0.4151747226715088\tSSIMM: 0.9478873014450073\tID: 0.001037179259583354\tTotal: 115.35518646240234\n",
      "[2000/200001]\tG: 0.0038312701508402824\tD: 1.00629460811615\tAge: 99.89787292480469\tL1: 0.3553234040737152\tSSIMM: 0.9045970439910889\tID: 0.0012939636362716556\tTotal: 140.5888671875\n",
      "[2050/200001]\tG: 0.00047015317250043154\tD: 0.9883708357810974\tAge: 27.72612762451172\tL1: 0.6249521374702454\tSSIMM: 0.949394166469574\tID: 0.0015855450183153152\tTotal: 109.9278564453125\n",
      "[2100/200001]\tG: 0.003885783487930894\tD: 1.0053828954696655\tAge: 15.708066940307617\tL1: 0.5022530555725098\tSSIMM: 0.9390816688537598\tID: 0.001266966573894024\tTotal: 106.82176971435547\n",
      "[2150/200001]\tG: 0.0002627763897180557\tD: 1.0096487998962402\tAge: 5.418912887573242\tL1: 0.5638271570205688\tSSIMM: 0.9405214786529541\tID: 0.0012114665005356073\tTotal: 103.1326904296875\n",
      "[2200/200001]\tG: 0.0008720842888578773\tD: 0.9794616103172302\tAge: 12.963190078735352\tL1: 0.48476600646972656\tSSIMM: 0.9268800616264343\tID: 0.0012648605043068528\tTotal: 103.1316909790039\n",
      "[2250/200001]\tG: 0.0003899069270119071\tD: 1.0023096799850464\tAge: 3.9391157627105713\tL1: 0.4714587926864624\tSSIMM: 0.9376060366630554\tID: 0.0013103736564517021\tTotal: 101.80686950683594\n",
      "[2300/200001]\tG: 0.002084597945213318\tD: 1.0278584957122803\tAge: 11.96169662475586\tL1: 0.42930135130882263\tSSIMM: 0.9405298829078674\tID: 0.002142682671546936\tTotal: 107.57095336914062\n",
      "[2350/200001]\tG: 0.011195873841643333\tD: 1.0387102365493774\tAge: 18.68778419494629\tL1: 0.24916665256023407\tSSIMM: 0.851022481918335\tID: 0.0006144654471427202\tTotal: 111.34626007080078\n",
      "[2400/200001]\tG: 0.0002501893322914839\tD: 0.9861988425254822\tAge: 14.023253440856934\tL1: 0.36506712436676025\tSSIMM: 0.9168787002563477\tID: 0.0007927824044600129\tTotal: 104.22933959960938\n",
      "[2450/200001]\tG: 0.0093808863312006\tD: 1.0252264738082886\tAge: 12.303845405578613\tL1: 0.37112635374069214\tSSIMM: 0.9227166175842285\tID: 0.0015370475593954325\tTotal: 107.44448852539062\n",
      "[2500/200001]\tG: 0.0003219284990336746\tD: 0.9983975887298584\tAge: 12.614985466003418\tL1: 0.47107580304145813\tSSIMM: 0.9314711689949036\tID: 0.002127475803717971\tTotal: 104.88618469238281\n",
      "[2550/200001]\tG: 0.00025468532112427056\tD: 0.9918780326843262\tAge: 50.402774810791016\tL1: 0.3899528384208679\tSSIMM: 0.9414082765579224\tID: 0.001347178127616644\tTotal: 119.34918212890625\n",
      "[2600/200001]\tG: 0.0008083777502179146\tD: 0.9886712431907654\tAge: 68.40416717529297\tL1: 0.46492674946784973\tSSIMM: 0.9247661232948303\tID: 0.0013418723829090595\tTotal: 126.22905731201172\n",
      "[2650/200001]\tG: 0.0002783009840641171\tD: 1.0219634771347046\tAge: 6.991968154907227\tL1: 0.5788223743438721\tSSIMM: 0.9340262413024902\tID: 0.001239829696714878\tTotal: 104.9933853149414\n",
      "[2700/200001]\tG: 0.0008354950114153326\tD: 1.0214149951934814\tAge: 3.7677040100097656\tL1: 0.5278955698013306\tSSIMM: 0.9627796411514282\tID: 0.0012081977911293507\tTotal: 103.6488265991211\n",
      "[2750/200001]\tG: 0.0011850709561258554\tD: 0.9848589897155762\tAge: 5.972193717956543\tL1: 0.4873293340206146\tSSIMM: 0.9328497052192688\tID: 0.0018141184700652957\tTotal: 100.8751449584961\n",
      "[2800/200001]\tG: 0.00030952098313719034\tD: 0.9898184537887573\tAge: 2.95468807220459\tL1: 0.6747997999191284\tSSIMM: 0.9775527715682983\tID: 0.003076074179261923\tTotal: 100.16433715820312\n",
      "[2850/200001]\tG: 0.0035466616973280907\tD: 0.9522336721420288\tAge: 1.0207605361938477\tL1: 0.5020354390144348\tSSIMM: 0.9310345649719238\tID: 0.002136821858584881\tTotal: 95.63209533691406\n",
      "[2900/200001]\tG: 0.005999693181365728\tD: 1.041144847869873\tAge: 2.0051398277282715\tL1: 0.46200454235076904\tSSIMM: 0.930820643901825\tID: 0.0016255523078143597\tTotal: 104.9168701171875\n",
      "[2950/200001]\tG: 0.002555279294028878\tD: 0.9839335083961487\tAge: 3.7693381309509277\tL1: 0.3280197083950043\tSSIMM: 0.8936614394187927\tID: 0.001053855288773775\tTotal: 99.90129852294922\n",
      "[3000/200001]\tG: 0.0007492803269997239\tD: 0.9983183145523071\tAge: 3.482250452041626\tL1: 0.4395759105682373\tSSIMM: 0.9340947270393372\tID: 0.0008083818247541785\tTotal: 101.22489166259766\n",
      "[3050/200001]\tG: 0.0006916027632541955\tD: 1.0208348035812378\tAge: 8.185050964355469\tL1: 0.36631911993026733\tSSIMM: 0.940819501876831\tID: 0.0017185285687446594\tTotal: 105.35784149169922\n",
      "[3100/200001]\tG: 0.0015635474119335413\tD: 1.0245281457901\tAge: 5.3862223625183105\tL1: 0.3457971513271332\tSSIMM: 0.9134052395820618\tID: 0.0006663217209279537\tTotal: 104.60742950439453\n",
      "[3150/200001]\tG: 0.009747790172696114\tD: 0.9217777848243713\tAge: 5.444704055786133\tL1: 0.43865522742271423\tSSIMM: 0.9604859948158264\tID: 0.0011054478818550706\tTotal: 94.35588073730469\n",
      "[3200/200001]\tG: 0.006018162705004215\tD: 0.9155804514884949\tAge: 25.567705154418945\tL1: 0.24646766483783722\tSSIMM: 0.8899632692337036\tID: 0.001227261614985764\tTotal: 101.78536987304688\n",
      "[3250/200001]\tG: 0.00025171914603561163\tD: 0.9946584701538086\tAge: 6.178949356079102\tL1: 0.2576808035373688\tSSIMM: 0.8724387884140015\tID: 0.0013236922677606344\tTotal: 101.93769836425781\n",
      "[3300/200001]\tG: 0.031190574169158936\tD: 0.6097949147224426\tAge: 21.000741958618164\tL1: 0.1722683310508728\tSSIMM: 0.8200539350509644\tID: 0.0013292235089465976\tTotal: 69.38005828857422\n",
      "[3350/200001]\tG: 0.0003438401035964489\tD: 0.9985539317131042\tAge: 4.082334518432617\tL1: 0.22828030586242676\tSSIMM: 0.8790781497955322\tID: 0.0007107621058821678\tTotal: 101.48847198486328\n",
      "[3400/200001]\tG: 0.009815682657063007\tD: 1.0294214487075806\tAge: 15.788955688476562\tL1: 0.18137840926647186\tSSIMM: 0.7948660254478455\tID: 0.0011357490438967943\tTotal: 109.2579574584961\n",
      "[3450/200001]\tG: 0.018548427149653435\tD: 0.9712221026420593\tAge: 58.77373123168945\tL1: 0.2874126136302948\tSSIMM: 0.8754575848579407\tID: 0.0013887900859117508\tTotal: 120.63197326660156\n",
      "[3500/200001]\tG: 0.01289148349314928\tD: 0.9890199899673462\tAge: 6.360123634338379\tL1: 0.17549669742584229\tSSIMM: 0.776790201663971\tID: 0.0008518443792127073\tTotal: 101.44622039794922\n",
      "[3550/200001]\tG: 0.0011304817162454128\tD: 0.9775511622428894\tAge: 6.096403121948242\tL1: 0.20184221863746643\tSSIMM: 0.8592422008514404\tID: 0.000501527450978756\tTotal: 100.19377899169922\n",
      "[3600/200001]\tG: 0.013631803914904594\tD: 0.8161816000938416\tAge: 19.75279998779297\tL1: 0.1678677797317505\tSSIMM: 0.8331029415130615\tID: 0.0007138191722333431\tTotal: 89.51942443847656\n",
      "[3650/200001]\tG: 0.05776945501565933\tD: 0.7465148568153381\tAge: 139.84927368164062\tL1: 0.12964887917041779\tSSIMM: 0.6309832334518433\tID: 0.0009065225021913648\tTotal: 130.5913848876953\n",
      "[3700/200001]\tG: 0.009853359311819077\tD: 0.9091219305992126\tAge: 12.49649429321289\tL1: 0.19069339334964752\tSSIMM: 0.80597984790802\tID: 0.0007740998407825828\tTotal: 95.91094207763672\n",
      "[3750/200001]\tG: 0.01525718905031681\tD: 1.0548049211502075\tAge: 118.12266540527344\tL1: 0.14918674528598785\tSSIMM: 0.7460931539535522\tID: 0.0006649094866588712\tTotal: 152.7296905517578\n",
      "[3800/200001]\tG: 0.02784636989235878\tD: 1.0193116664886475\tAge: 39.842342376708984\tL1: 0.15642587840557098\tSSIMM: 0.8030376434326172\tID: 0.0009130459511652589\tTotal: 117.8682861328125\n",
      "[3850/200001]\tG: 0.004713690839707851\tD: 0.9821579456329346\tAge: 11.723027229309082\tL1: 0.1349479854106903\tSSIMM: 0.7659192085266113\tID: 0.0006726896390318871\tTotal: 102.90514373779297\n",
      "[3900/200001]\tG: 0.004198255483061075\tD: 0.8971966505050659\tAge: 60.37902069091797\tL1: 0.1627252995967865\tSSIMM: 0.7375204563140869\tID: 0.0014097616076469421\tTotal: 113.8715591430664\n",
      "[3950/200001]\tG: 0.13473758101463318\tD: 0.7397789359092712\tAge: 41.062286376953125\tL1: 0.17630288004875183\tSSIMM: 0.739820122718811\tID: 0.0016430068062618375\tTotal: 90.40312957763672\n",
      "[4000/200001]\tG: 0.011269403621554375\tD: 1.125321626663208\tAge: 78.18251037597656\tL1: 0.09100291132926941\tSSIMM: 0.5502667427062988\tID: 0.0008001163951121271\tTotal: 143.80532836914062\n",
      "[4050/200001]\tG: 0.002235474530607462\tD: 0.953464150428772\tAge: 20.085575103759766\tL1: 0.17120565474033356\tSSIMM: 0.7696729302406311\tID: 0.0016946654068306088\tTotal: 103.3809814453125\n",
      "[4100/200001]\tG: 0.06557564437389374\tD: 0.598607063293457\tAge: 25.27959632873535\tL1: 0.14979951083660126\tSSIMM: 0.7486939430236816\tID: 0.0018508220091462135\tTotal: 69.97291564941406\n",
      "[4150/200001]\tG: 0.041253868490457535\tD: 0.7384365200996399\tAge: 13.438814163208008\tL1: 0.13383078575134277\tSSIMM: 0.6527407169342041\tID: 0.0007082093507051468\tTotal: 79.21932220458984\n",
      "[4200/200001]\tG: 0.015660936012864113\tD: 1.0441182851791382\tAge: 16.422544479370117\tL1: 0.1077929139137268\tSSIMM: 0.6072441935539246\tID: 0.000695676077157259\tTotal: 110.9809799194336\n",
      "[4250/200001]\tG: 0.01325612235814333\tD: 1.1405481100082397\tAge: 53.72727966308594\tL1: 0.10465839505195618\tSSIMM: 0.6103470921516418\tID: 0.0011311981361359358\tTotal: 135.5459442138672\n",
      "[4300/200001]\tG: 0.0031294359359890223\tD: 1.071390151977539\tAge: 15.233487129211426\tL1: 0.1223311573266983\tSSIMM: 0.6744825839996338\tID: 0.0009157119784504175\tTotal: 113.23258972167969\n",
      "[4350/200001]\tG: 0.0014433034230023623\tD: 0.9693212509155273\tAge: 20.168617248535156\tL1: 0.15507060289382935\tSSIMM: 0.82370924949646\tID: 0.0007694704690948129\tTotal: 104.99972534179688\n",
      "[4400/200001]\tG: 0.054139718413352966\tD: 0.6360344886779785\tAge: 14.030311584472656\tL1: 0.10832816362380981\tSSIMM: 0.7184901237487793\tID: 0.0007798938313499093\tTotal: 69.2157211303711\n",
      "[4450/200001]\tG: 0.002730592153966427\tD: 1.046609878540039\tAge: 23.462425231933594\tL1: 0.10259843617677689\tSSIMM: 0.6571193933486938\tID: 0.0006926099304109812\tTotal: 114.04609680175781\n",
      "[4500/200001]\tG: 0.002165817888453603\tD: 1.0079458951950073\tAge: 22.626384735107422\tL1: 0.10588160157203674\tSSIMM: 0.6132718324661255\tID: 0.0007926750695332885\tTotal: 109.84529876708984\n",
      "[4550/200001]\tG: 0.0022910251282155514\tD: 0.9994708299636841\tAge: 41.617401123046875\tL1: 0.1388348937034607\tSSIMM: 0.7657743096351624\tID: 0.000564854359254241\tTotal: 116.59415435791016\n",
      "[4600/200001]\tG: 0.007758928928524256\tD: 1.016340970993042\tAge: 11.631175994873047\tL1: 0.1568274050951004\tSSIMM: 0.7715713977813721\tID: 0.0010771176312118769\tTotal: 106.28678131103516\n",
      "[4650/200001]\tG: 0.024960828945040703\tD: 1.031554102897644\tAge: 27.130634307861328\tL1: 0.08759865909814835\tSSIMM: 0.6612938642501831\tID: 0.0005394567269831896\tTotal: 114.00777435302734\n",
      "[4700/200001]\tG: 0.00139370106626302\tD: 1.0104070901870728\tAge: 12.198587417602539\tL1: 0.12060682475566864\tSSIMM: 0.6344326138496399\tID: 0.00033089081989601254\tTotal: 105.92021179199219\n",
      "[4750/200001]\tG: 0.044627461582422256\tD: 0.6457287073135376\tAge: 36.62776565551758\tL1: 0.10735229402780533\tSSIMM: 0.6984239816665649\tID: 0.0006912652170285583\tTotal: 79.22411346435547\n",
      "[4800/200001]\tG: 0.003382594557479024\tD: 0.9599606990814209\tAge: 48.77863311767578\tL1: 0.12536582350730896\tSSIMM: 0.702862024307251\tID: 0.0006781931733712554\tTotal: 115.50765991210938\n",
      "[4850/200001]\tG: 0.0016643828712403774\tD: 1.0105605125427246\tAge: 14.364615440368652\tL1: 0.11459670960903168\tSSIMM: 0.6978603601455688\tID: 0.0005235105054453015\tTotal: 106.80200958251953\n",
      "[4900/200001]\tG: 0.0023524793796241283\tD: 1.0202609300613403\tAge: 67.55729675292969\tL1: 0.10958968102931976\tSSIMM: 0.6849216222763062\tID: 0.00044212338980287313\tTotal: 129.04910278320312\n",
      "[4950/200001]\tG: 0.0017880925443023443\tD: 0.9627651572227478\tAge: 18.682369232177734\tL1: 0.10631672292947769\tSSIMM: 0.6776276230812073\tID: 0.0005050471518188715\tTotal: 103.74955749511719\n",
      "[5000/200001]\tG: 0.002637797500938177\tD: 0.9459497928619385\tAge: 39.10533905029297\tL1: 0.12356004118919373\tSSIMM: 0.6934080123901367\tID: 0.0007057498442009091\tTotal: 110.23725891113281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/js07987/.local/lib/python3.11/site-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
      "  warnings.warn(\"An input tensor was not cuda.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5050/200001]\tG: 0.006473284214735031\tD: 0.9767396450042725\tAge: 30.508134841918945\tL1: 0.09990426898002625\tSSIMM: 0.6555522680282593\tID: 0.0005285386578179896\tTotal: 109.87732696533203\n",
      "[5100/200001]\tG: 0.09289022535085678\tD: 0.9580775499343872\tAge: 20.791719436645508\tL1: 0.10099288076162338\tSSIMM: 0.6315279006958008\tID: 0.0006581018678843975\tTotal: 104.12457275390625\n",
      "[5150/200001]\tG: 0.01810409687459469\tD: 0.7001225352287292\tAge: 40.1829948425293\tL1: 0.12080104649066925\tSSIMM: 0.7061196565628052\tID: 0.0006254188483580947\tTotal: 86.0855712890625\n",
      "[5200/200001]\tG: 0.03900328278541565\tD: 0.6387027502059937\tAge: 8.725637435913086\tL1: 0.09609228372573853\tSSIMM: 0.5966297388076782\tID: 0.0005841910606250167\tTotal: 67.36064147949219\n",
      "[5250/200001]\tG: 0.004072193056344986\tD: 1.0180835723876953\tAge: 34.8148078918457\tL1: 0.10221044719219208\tSSIMM: 0.6860346794128418\tID: 0.0006912681856192648\tTotal: 115.73442077636719\n",
      "[5300/200001]\tG: 0.04622208699584007\tD: 1.1123871803283691\tAge: 19.508440017700195\tL1: 0.12503182888031006\tSSIMM: 0.692141056060791\tID: 0.0005857403157278895\tTotal: 119.04220581054688\n",
      "[5350/200001]\tG: 0.12415511161088943\tD: 0.8639943599700928\tAge: 158.41392517089844\tL1: 0.0992191955447197\tSSIMM: 0.6174072027206421\tID: 0.000586968963034451\tTotal: 149.76513671875\n",
      "[5400/200001]\tG: 0.0043999208137393\tD: 0.9676580429077148\tAge: 53.28160095214844\tL1: 0.11559689790010452\tSSIMM: 0.7129772901535034\tID: 0.0005529423942789435\tTotal: 118.07855224609375\n",
      "[5450/200001]\tG: 0.004103355575352907\tD: 0.9804702997207642\tAge: 8.969833374023438\tL1: 0.09989507496356964\tSSIMM: 0.5944982767105103\tID: 0.00047332473332062364\tTotal: 101.63505554199219\n",
      "[5500/200001]\tG: 0.0019846241921186447\tD: 0.990158200263977\tAge: 23.777456283569336\tL1: 0.09712459146976471\tSSIMM: 0.6603072881698608\tID: 0.000426699873059988\tTotal: 108.52689361572266\n",
      "[5550/200001]\tG: 0.00517860846593976\tD: 1.0007944107055664\tAge: 39.366737365722656\tL1: 0.12029707431793213\tSSIMM: 0.6622142791748047\tID: 0.0006986032240092754\tTotal: 115.8262710571289\n",
      "[5600/200001]\tG: 0.028109876438975334\tD: 0.9194614291191101\tAge: 26.719675064086914\tL1: 0.14056982100009918\tSSIMM: 0.7262688875198364\tID: 0.0007819893071427941\tTotal: 102.63417053222656\n",
      "[5650/200001]\tG: 0.03293194621801376\tD: 0.8749831914901733\tAge: 34.283912658691406\tL1: 0.13797925412654877\tSSIMM: 0.7520383596420288\tID: 0.0004897734033875167\tTotal: 101.21198272705078\n",
      "[5700/200001]\tG: 0.00299609312787652\tD: 1.0674906969070435\tAge: 5.394466400146484\tL1: 0.1117473691701889\tSSIMM: 0.6737351417541504\tID: 0.0005108697805553675\tTotal: 108.90695190429688\n",
      "[5750/200001]\tG: 0.000532988109625876\tD: 0.9946125149726868\tAge: 12.122024536132812\tL1: 0.10508956015110016\tSSIMM: 0.6868377923965454\tID: 0.00032603865838609636\tTotal: 104.31012725830078\n",
      "[5800/200001]\tG: 0.0014869668520987034\tD: 1.0053048133850098\tAge: 11.83597183227539\tL1: 0.11627089232206345\tSSIMM: 0.6765071749687195\tID: 0.00039139186264947057\tTotal: 105.26494598388672\n",
      "[5850/200001]\tG: 0.0012446949258446693\tD: 1.0324167013168335\tAge: 16.726863861083984\tL1: 0.13017284870147705\tSSIMM: 0.7232333421707153\tID: 0.00045892613707110286\tTotal: 109.93250274658203\n",
      "[5900/200001]\tG: 0.004518750123679638\tD: 1.0802185535430908\tAge: 17.995704650878906\tL1: 0.15324151515960693\tSSIMM: 0.8566816449165344\tID: 0.0007995962514542043\tTotal: 115.22029876708984\n",
      "[5950/200001]\tG: 0.08347298204898834\tD: 1.2264379262924194\tAge: 6.129478454589844\tL1: 0.11183514446020126\tSSIMM: 0.716510534286499\tID: 0.0006700651138089597\tTotal: 125.09571838378906\n",
      "[6000/200001]\tG: 0.0012628624681383371\tD: 0.9841271042823792\tAge: 12.720715522766113\tL1: 0.13184578716754913\tSSIMM: 0.7074406743049622\tID: 0.00044562056427821517\tTotal: 103.50109100341797\n",
      "[6050/200001]\tG: 0.007416843436658382\tD: 1.0435938835144043\tAge: 31.700998306274414\tL1: 0.12614572048187256\tSSIMM: 0.738816499710083\tID: 0.0004717019619420171\tTotal: 117.03987884521484\n",
      "[6100/200001]\tG: 0.0008625606424175203\tD: 1.0040770769119263\tAge: 8.817574501037598\tL1: 0.14785242080688477\tSSIMM: 0.8155581951141357\tID: 0.00044532696483656764\tTotal: 103.93482971191406\n",
      "[6150/200001]\tG: 0.0006872470839880407\tD: 1.0135043859481812\tAge: 5.731602668762207\tL1: 0.15353482961654663\tSSIMM: 0.8035311698913574\tID: 0.0006134973373264074\tTotal: 103.64320373535156\n",
      "[6200/200001]\tG: 0.0021263814996927977\tD: 1.029241919517517\tAge: 11.996938705444336\tL1: 0.18427808582782745\tSSIMM: 0.8501377105712891\tID: 0.0007381222676485777\tTotal: 107.72311401367188\n",
      "[6250/200001]\tG: 0.001023941906169057\tD: 0.9758116006851196\tAge: 3.426997423171997\tL1: 0.15068887174129486\tSSIMM: 0.7840861082077026\tID: 0.0006643153028562665\tTotal: 98.95208740234375\n",
      "[6300/200001]\tG: 0.006410228554159403\tD: 1.1469835042953491\tAge: 128.3834991455078\tL1: 0.21370230615139008\tSSIMM: 0.6954973936080933\tID: 0.0006212982116267085\tTotal: 166.0518798828125\n",
      "[6350/200001]\tG: 0.0008874294580891728\tD: 1.0331268310546875\tAge: 8.50047779083252\tL1: 0.20718039572238922\tSSIMM: 0.8216289281845093\tID: 0.0007935129106044769\tTotal: 106.7130355834961\n",
      "[6400/200001]\tG: 0.030185461044311523\tD: 0.858489453792572\tAge: 27.534740447998047\tL1: 0.11562901735305786\tSSIMM: 0.6506320238113403\tID: 0.0003620747593231499\tTotal: 96.86290740966797\n",
      "[6450/200001]\tG: 0.0017101230332627892\tD: 0.9706330895423889\tAge: 5.192495346069336\tL1: 0.1409769207239151\tSSIMM: 0.8144018650054932\tID: 0.0007449478143826127\tTotal: 99.14045715332031\n",
      "[6500/200001]\tG: 0.005232810974121094\tD: 0.8950971961021423\tAge: 6.833878993988037\tL1: 0.15778644382953644\tSSIMM: 0.821392297744751\tID: 0.0006133834831416607\tTotal: 92.24339294433594\n",
      "[6550/200001]\tG: 0.003604687051847577\tD: 0.995177149772644\tAge: 2.3477330207824707\tL1: 0.154320627450943\tSSIMM: 0.8285059928894043\tID: 0.0005470017204061151\tTotal: 100.45691680908203\n",
      "[6600/200001]\tG: 0.0010253161890432239\tD: 1.0120017528533936\tAge: 12.754571914672852\tL1: 0.1611311286687851\tSSIMM: 0.76265549659729\tID: 0.00039972609374672174\tTotal: 106.30207824707031\n",
      "[6650/200001]\tG: 0.0003567130770534277\tD: 1.0009247064590454\tAge: 5.3504180908203125\tL1: 0.1578177660703659\tSSIMM: 0.8427032232284546\tID: 0.0006835137610323727\tTotal: 102.23277282714844\n",
      "[6700/200001]\tG: 0.0008382704108953476\tD: 0.9880756735801697\tAge: 9.455991744995117\tL1: 0.17646248638629913\tSSIMM: 0.8202295303344727\tID: 0.00044273288222029805\tTotal: 102.59004974365234\n",
      "[6750/200001]\tG: 0.000845518778078258\tD: 0.9831165671348572\tAge: 2.9124321937561035\tL1: 0.19990357756614685\tSSIMM: 0.8289178609848022\tID: 0.0006162442732602358\tTotal: 99.47674560546875\n",
      "[6800/200001]\tG: 0.0014901179820299149\tD: 1.0155028104782104\tAge: 3.533971071243286\tL1: 0.21384957432746887\tSSIMM: 0.8743913173675537\tID: 0.0005126537289470434\tTotal: 102.9639663696289\n",
      "[6850/200001]\tG: 0.0012018433772027493\tD: 1.0231744050979614\tAge: 38.35662841796875\tL1: 0.15506985783576965\tSSIMM: 0.7911989688873291\tID: 0.0005127580370754004\tTotal: 117.66019439697266\n",
      "[6900/200001]\tG: 0.08232258260250092\tD: 1.645250916481018\tAge: 10.987905502319336\tL1: 0.15609110891819\tSSIMM: 0.8049615621566772\tID: 0.0005417602951638401\tTotal: 168.92034912109375\n",
      "[6950/200001]\tG: 0.000438322895206511\tD: 1.0019131898880005\tAge: 8.785578727722168\tL1: 0.18349985778331757\tSSIMM: 0.8012869954109192\tID: 0.0004734438844025135\tTotal: 103.70564270019531\n",
      "[7000/200001]\tG: 0.0017074954230338335\tD: 1.0143412351608276\tAge: 1.8282297849655151\tL1: 0.20256561040878296\tSSIMM: 0.8375738859176636\tID: 0.0008285926887765527\tTotal: 102.16558074951172\n",
      "[7050/200001]\tG: 0.0007702605798840523\tD: 0.9985411167144775\tAge: 2.645610809326172\tL1: 0.17365007102489471\tSSIMM: 0.8002713918685913\tID: 0.0006775001529604197\tTotal: 100.91249084472656\n",
      "[7100/200001]\tG: 0.004942517261952162\tD: 0.9819009304046631\tAge: 3.60105299949646\tL1: 0.14888995885849\tSSIMM: 0.7199094295501709\tID: 0.0005063886055722833\tTotal: 99.630615234375\n",
      "[7150/200001]\tG: 0.0006478517316281796\tD: 1.0227954387664795\tAge: 1.4889535903930664\tL1: 0.16324304044246674\tSSIMM: 0.7615172266960144\tID: 0.0005346313118934631\tTotal: 102.87522888183594\n",
      "[7200/200001]\tG: 0.00067050353391096\tD: 0.9868283271789551\tAge: 34.02674102783203\tL1: 0.20957617461681366\tSSIMM: 0.8468517661094666\tID: 0.0008075099904090166\tTotal: 112.29368591308594\n",
      "[7250/200001]\tG: 0.011651314795017242\tD: 0.8677239418029785\tAge: 7.018063545227051\tL1: 0.13366247713565826\tSSIMM: 0.7626575231552124\tID: 0.0006036045379005373\tTotal: 89.57974243164062\n",
      "[7300/200001]\tG: 0.0011020320234820247\tD: 0.98681640625\tAge: 2.598815679550171\tL1: 0.15909557044506073\tSSIMM: 0.7817189693450928\tID: 0.0008666922803968191\tTotal: 99.72134399414062\n",
      "[7350/200001]\tG: 0.001168806222267449\tD: 0.9697693586349487\tAge: 4.295216083526611\tL1: 0.15649697184562683\tSSIMM: 0.8017007112503052\tID: 0.0006506647914648056\tTotal: 98.69515228271484\n",
      "[7400/200001]\tG: 0.07955323159694672\tD: 1.4718658924102783\tAge: 5.394193649291992\tL1: 0.16036586463451385\tSSIMM: 0.8339143991470337\tID: 0.0006470274529419839\tTotal: 149.34439086914062\n",
      "[7450/200001]\tG: 0.0036890089977532625\tD: 0.9690291881561279\tAge: 10.613685607910156\tL1: 0.12854453921318054\tSSIMM: 0.7446091771125793\tID: 0.00045572713133879006\tTotal: 101.14848327636719\n",
      "[7500/200001]\tG: 0.0025694638025015593\tD: 1.0556654930114746\tAge: 7.358493804931641\tL1: 0.17142951488494873\tSSIMM: 0.8292748928070068\tID: 0.0006063974578864872\tTotal: 108.51007080078125\n",
      "[7550/200001]\tG: 0.004887314513325691\tD: 0.9802047610282898\tAge: 16.657100677490234\tL1: 0.20210909843444824\tSSIMM: 0.8719273805618286\tID: 0.0007014998700469732\tTotal: 104.68345642089844\n",
      "[7600/200001]\tG: 0.000583679648116231\tD: 0.9896022081375122\tAge: 3.512708902359009\tL1: 0.1559949368238449\tSSIMM: 0.8134270906448364\tID: 0.000653774244710803\tTotal: 100.36543273925781\n",
      "[7650/200001]\tG: 0.0005232149851508439\tD: 1.0086349248886108\tAge: 32.460323333740234\tL1: 0.1660148948431015\tSSIMM: 0.7981823086738586\tID: 0.0007183864945545793\tTotal: 113.84777069091797\n",
      "[7700/200001]\tG: 0.0008029966265894473\tD: 1.025766134262085\tAge: 5.058000087738037\tL1: 0.20864327251911163\tSSIMM: 0.8191376328468323\tID: 0.0007666316814720631\tTotal: 104.59996795654297\n",
      "[7750/200001]\tG: 0.000818536733277142\tD: 0.9902955889701843\tAge: 5.5806121826171875\tL1: 0.18462616205215454\tSSIMM: 0.7390695214271545\tID: 0.000356345932232216\tTotal: 101.26187133789062\n",
      "[7800/200001]\tG: 0.004060695413500071\tD: 0.9975422024726868\tAge: 0.9273797273635864\tL1: 0.2173227071762085\tSSIMM: 0.8321906924247742\tID: 0.0006419631536118686\tTotal: 100.12529754638672\n",
      "[7850/200001]\tG: 0.00043030365486629307\tD: 0.9934311509132385\tAge: 6.960538387298584\tL1: 0.22452951967716217\tSSIMM: 0.8323624730110168\tID: 0.0005057496018707752\tTotal: 102.12743377685547\n",
      "[7900/200001]\tG: 0.0006052803946658969\tD: 0.9893921613693237\tAge: 4.40235710144043\tL1: 0.14810825884342194\tSSIMM: 0.7766821384429932\tID: 0.000598774990066886\tTotal: 100.70027923583984\n",
      "[7950/200001]\tG: 0.0006903515895828605\tD: 1.0263934135437012\tAge: 6.378425598144531\tL1: 0.18281157314777374\tSSIMM: 0.8066470623016357\tID: 0.00036549230571836233\tTotal: 105.19078826904297\n",
      "[8000/200001]\tG: 0.0023291194811463356\tD: 1.090255618095398\tAge: 2.650702476501465\tL1: 0.24360516667366028\tSSIMM: 0.7785775661468506\tID: 0.0008665372151881456\tTotal: 110.08601379394531\n",
      "[8050/200001]\tG: 0.001263309270143509\tD: 0.9701364040374756\tAge: 46.88969802856445\tL1: 0.18310226500034332\tSSIMM: 0.8117356300354004\tID: 0.0006321045802906156\tTotal: 115.7696533203125\n",
      "[8100/200001]\tG: 0.012055562809109688\tD: 1.0166677236557007\tAge: 5.513019561767578\tL1: 0.17917375266551971\tSSIMM: 0.815197765827179\tID: 0.0005265421350486577\tTotal: 103.87208557128906\n",
      "[8150/200001]\tG: 0.012524849735200405\tD: 0.9295343160629272\tAge: 12.155385971069336\tL1: 0.17167231440544128\tSSIMM: 0.7686451077461243\tID: 0.0008193284156732261\tTotal: 97.81574249267578\n",
      "[8200/200001]\tG: 0.0007153179030865431\tD: 0.9926522970199585\tAge: 1.9698402881622314\tL1: 0.16526348888874054\tSSIMM: 0.8016312718391418\tID: 0.0007487112889066339\tTotal: 100.05331420898438\n",
      "[8250/200001]\tG: 0.024481933563947678\tD: 0.9864640235900879\tAge: 4.212663173675537\tL1: 0.14991407096385956\tSSIMM: 0.704028308391571\tID: 0.0006080429302528501\tTotal: 100.33158874511719\n",
      "[8300/200001]\tG: 0.0018049267819151282\tD: 0.9724146723747253\tAge: 5.081266403198242\tL1: 0.13797403872013092\tSSIMM: 0.7896574139595032\tID: 0.0008352488512173295\tTotal: 99.2741470336914\n",
      "[8350/200001]\tG: 0.0007088020793162286\tD: 1.0041359663009644\tAge: 11.28526496887207\tL1: 0.1811276078224182\tSSIMM: 0.7792289853096008\tID: 0.0006843613227829337\tTotal: 104.92784118652344\n",
      "[8400/200001]\tG: 0.004330171272158623\tD: 1.0502454042434692\tAge: 13.000680923461914\tL1: 0.17193597555160522\tSSIMM: 0.7004157304763794\tID: 0.00047406606609001756\tTotal: 110.22490692138672\n",
      "[8450/200001]\tG: 0.0011214475380256772\tD: 1.0290700197219849\tAge: 27.139205932617188\tL1: 0.13335548341274261\tSSIMM: 0.7572855949401855\tID: 0.0005756893660873175\tTotal: 113.76280212402344\n",
      "[8500/200001]\tG: 0.005781063809990883\tD: 1.080478310585022\tAge: 15.932308197021484\tL1: 0.12315307557582855\tSSIMM: 0.700271487236023\tID: 0.0004299526335671544\tTotal: 114.42083740234375\n",
      "[8550/200001]\tG: 0.010079543106257915\tD: 1.1339114904403687\tAge: 7.271272659301758\tL1: 0.16094769537448883\tSSIMM: 0.7804174423217773\tID: 0.0005900016985833645\tTotal: 116.29977416992188\n",
      "[8600/200001]\tG: 0.0019069116096943617\tD: 0.9818695783615112\tAge: 52.950408935546875\tL1: 0.10909976065158844\tSSIMM: 0.6368528604507446\tID: 0.0006102679762989283\tTotal: 119.36724853515625\n",
      "[8650/200001]\tG: 0.0893741324543953\tD: 0.6643551588058472\tAge: 4.885584831237793\tL1: 0.10514137893915176\tSSIMM: 0.6466468572616577\tID: 0.0005328285624273121\tTotal: 68.38985443115234\n",
      "[8700/200001]\tG: 0.029245611280202866\tD: 0.8154547810554504\tAge: 10.5946044921875\tL1: 0.11727055162191391\tSSIMM: 0.6720899343490601\tID: 0.000602089217863977\tTotal: 85.78343963623047\n",
      "[8750/200001]\tG: 0.008021926507353783\tD: 0.9808230400085449\tAge: 18.318161010742188\tL1: 0.10760924220085144\tSSIMM: 0.5845749378204346\tID: 0.00036939745768904686\tTotal: 105.4096450805664\n",
      "[8800/200001]\tG: 0.3268866240978241\tD: 0.8399896621704102\tAge: 12.933932304382324\tL1: 0.14905567467212677\tSSIMM: 0.7445578575134277\tID: 0.0006823864532634616\tTotal: 89.17267608642578\n",
      "[8850/200001]\tG: 0.002858524676412344\tD: 0.9849598407745361\tAge: 48.52448272705078\tL1: 0.13486142456531525\tSSIMM: 0.77284836769104\tID: 0.0007152993930503726\tTotal: 117.90592193603516\n",
      "[8900/200001]\tG: 0.0012345293071120977\tD: 1.0160635709762573\tAge: 20.661334991455078\tL1: 0.11133746057748795\tSSIMM: 0.6662091016769409\tID: 0.00043242130777798593\tTotal: 109.8709716796875\n",
      "[8950/200001]\tG: 0.3419273793697357\tD: 0.4641355276107788\tAge: 19.234201431274414\tL1: 0.0980565994977951\tSSIMM: 0.5720615386962891\tID: 0.0003566293162293732\tTotal: 54.107303619384766\n",
      "[9000/200001]\tG: 0.002778105903416872\tD: 0.9126961827278137\tAge: 10.255943298339844\tL1: 0.11357289552688599\tSSIMM: 0.6517742872238159\tID: 0.00028277107048779726\tTotal: 95.3720474243164\n",
      "[9050/200001]\tG: 0.006355364341288805\tD: 1.0771490335464478\tAge: 8.987707138061523\tL1: 0.15021805465221405\tSSIMM: 0.7439188361167908\tID: 0.00076187594095245\tTotal: 111.31014251708984\n",
      "[9100/200001]\tG: 0.2374591827392578\tD: 0.56996750831604\tAge: 24.799043655395508\tL1: 0.11993353068828583\tSSIMM: 0.6458413600921631\tID: 0.0005592440138570964\tTotal: 66.9164810180664\n",
      "[9150/200001]\tG: 0.0064381081610918045\tD: 0.925187885761261\tAge: 8.192313194274902\tL1: 0.11661046743392944\tSSIMM: 0.6852997541427612\tID: 0.00035602785646915436\tTotal: 95.79578399658203\n",
      "[9200/200001]\tG: 0.0006897194543853402\tD: 0.9822307229042053\tAge: 22.836153030395508\tL1: 0.10277688503265381\tSSIMM: 0.6417357325553894\tID: 0.00039848804590292275\tTotal: 107.35761260986328\n",
      "[9250/200001]\tG: 0.002584507456049323\tD: 1.008323073387146\tAge: 12.838403701782227\tL1: 0.11088024824857712\tSSIMM: 0.6828032732009888\tID: 0.00022448964591603726\tTotal: 105.96771240234375\n",
      "[9300/200001]\tG: 0.0004954342148266733\tD: 0.9917632341384888\tAge: 12.565546035766602\tL1: 0.11629486083984375\tSSIMM: 0.7144067287445068\tID: 0.0005844639963470399\tTotal: 104.2026596069336\n"
     ]
    }
   ],
   "source": [
    "pfa_gan.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ead1b95f633dc9c51826328e1846203f51a198c6fb5f2884a80417ba131d4e82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
